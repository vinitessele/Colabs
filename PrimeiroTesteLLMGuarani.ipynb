{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1VEZfYucJPQce1pHIh3YG1WLTEVtcbala","authorship_tag":"ABX9TyPfRJHBKBJ5avuVmHl7WUP9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Instalar bibliotecas necess√°rias\n","!pip install transformers datasets torch accelerate\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q8hXhYNCk0vW","executionInfo":{"status":"ok","timestamp":1748096506114,"user_tz":180,"elapsed":4275,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"44e0130b-228c-46aa-94a5-2c22126c9197"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}]},{"cell_type":"code","source":["\n","# 1. Montar o Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 2. Caminhos dos arquivos\n","caminho_entrada = '/content/drive/MyDrive/Doutorado Unesp/IA/projetoLLM/palavras.json'\n","caminho_saida = '/content/drive/MyDrive/Doutorado Unesp/IA/projetoLLM/dados_formatados.json'\n","caminho_modelo_final = '/content/drive/MyDrive/Doutorado Unesp/IA/projetoLLM/guarani-pt-model'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WP5HxPQml-Bi","executionInfo":{"status":"ok","timestamp":1748096806860,"user_tz":180,"elapsed":1352,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"b79b373b-1866-42a0-f701-608cc8151a2f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["with open(caminho_entrada, 'r', encoding='utf-8') as f:\n","    dados = json.load(f)\n","\n","print(f\"Total de itens carregados: {len(dados)}\")\n","print(\"Primeiro item:\")\n","print(dados[0] if len(dados) > 0 else \"JSON vazio\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"xAC8V8Wqm6Xs","executionInfo":{"status":"error","timestamp":1748096907597,"user_tz":180,"elapsed":61,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"0bc91fd4-5e92-4c2c-9e1d-c39a64b3ed9f"},"execution_count":12,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'json' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-f30d4b198583>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaminho_entrada\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total de itens carregados: {len(dados)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Primeiro item:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"]}]},{"cell_type":"code","source":["# 3. Importar bibliotecas\n","import json\n","import torch\n","from transformers import (\n","    AutoTokenizer, AutoConfig, AutoModelForCausalLM,\n","    Trainer, TrainingArguments, DataCollatorForLanguageModeling,\n","    GPT2Tokenizer, GPT2LMHeadModel\n",")\n","from datasets import Dataset, DatasetDict\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","# 4. Carregar e processar JSON original\n","print(\"Carregando dados...\")\n","with open(caminho_entrada, 'r', encoding='utf-8') as f:\n","    dados = json.load(f)\n","\n","print(f\"Total de registros encontrados: {len(dados)}\")\n","\n","# 5. Formatar para prompt/completion\n","formatted_data = []\n","for item in dados[\"frases\"]:  # <-- ajuste aqui\n","    if 'guarani' in item and 'portugues' in item:\n","        formatted_data.append({\n","            \"text\": f\"Traduza do guarani para o portugu√™s: {item['guarani']} -> {item['portugues']}\"\n","        })\n","print(f\"Total de exemplos formatados: {len(formatted_data)}\")\n","\n","# 6. Dividir dados em treino e valida√ß√£o\n","train_data, val_data = train_test_split(formatted_data, test_size=0.1, random_state=42)\n","print(f\"Dados de treino: {len(train_data)}, Valida√ß√£o: {len(val_data)}\")\n","\n"],"metadata":{"id":"U59s8_eCmEju","executionInfo":{"status":"aborted","timestamp":1748096508474,"user_tz":180,"elapsed":6931,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7. Salvar dados formatados\n","with open(caminho_saida, 'w', encoding='utf-8') as f_out:\n","    json.dump({\n","        'train': train_data,\n","        'validation': val_data,\n","        'total': len(formatted_data)\n","    }, f_out, ensure_ascii=False, indent=2)\n","\n","# 8. Criar Datasets HuggingFace\n","train_dataset = Dataset.from_list(train_data)\n","val_dataset = Dataset.from_list(val_data)\n","\n","# 9. Configurar tokenizer\n","print(\"Configurando tokenizer...\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Adicionar tokens especiais para melhor performance\n","special_tokens = {\n","    \"additional_special_tokens\": [\"<GUARANI>\", \"<PORTUGU√äS>\", \"<TRADU√á√ÉO>\"]\n","}\n","tokenizer.add_special_tokens(special_tokens)\n","\n","# 10. Fun√ß√£o de tokeniza√ß√£o otimizada\n","def tokenize_function(examples):\n","    # Usar comprimento din√¢mico baseado nos dados\n","    return tokenizer(\n","        examples[\"text\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=128,  # Aumentado para acomodar frases mais longas\n","        return_special_tokens_mask=True\n","    )\n","\n","print(\"Tokenizando dados...\")\n","tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n","tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n","\n","# 11. Configurar modelo com arquitetura otimizada\n","print(\"Configurando modelo...\")\n","config = AutoConfig.from_pretrained(\n","    \"gpt2\",\n","    vocab_size=len(tokenizer),\n","    n_layer=6,        # Aumentado para melhor capacidade\n","    n_embd=384,       # Embedding maior\n","    n_head=12,        # Mais cabe√ßas de aten√ß√£o\n","    n_positions=128,  # Sequ√™ncias mais longas\n","    bos_token_id=tokenizer.bos_token_id,\n","    eos_token_id=tokenizer.eos_token_id,\n","    pad_token_id=tokenizer.pad_token_id,\n",")\n","\n","model = GPT2LMHeadModel(config)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","print(f\"Modelo criado com {model.num_parameters():,} par√¢metros\")\n","\n","# 12. Data collator otimizado\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n","    return_tensors=\"pt\"\n",")\n","\n"],"metadata":{"id":"jMlKOQz-mdOw","executionInfo":{"status":"aborted","timestamp":1748096508522,"user_tz":180,"elapsed":1,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 13. Argumentos de treinamento otimizados\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","from transformers import Trainer,TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=5,\n","    per_device_train_batch_size=4,\n","    save_steps=500,\n","    save_total_limit=2,\n","    logging_steps=50,\n",")\n","\n","\n","# 14. Fun√ß√£o de m√©tricas customizada\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    # Calcular perplexidade\n","    shift_logits = predictions[..., :-1, :].contiguous()\n","    shift_labels = labels[..., 1:].contiguous()\n","    loss_fct = torch.nn.CrossEntropyLoss()\n","    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","    perplexity = torch.exp(loss)\n","    return {\"perplexity\": perplexity.item()}\n","\n","# 15. Criar e configurar Trainer\n","print(\"Iniciando treinamento...\")\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_val,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# 16. Treinar modelo\n","trainer.train()\n","\n"],"metadata":{"id":"U3giyCLYn4_H","executionInfo":{"status":"aborted","timestamp":1748096508527,"user_tz":180,"elapsed":3,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 17. Salvar modelo final\n","print(\"Salvando modelo...\")\n","model.save_pretrained(caminho_modelo_final)\n","tokenizer.save_pretrained(caminho_modelo_final)\n","\n","# Salvar tamb√©m localmente\n","model.save_pretrained(\"./guarani-pt-model-final\")\n","tokenizer.save_pretrained(\"./guarani-pt-model-final\")\n","\n","print(\"Treinamento conclu√≠do!\")\n","\n","# 18. Fun√ß√£o de tradu√ß√£o melhorada\n","def traduzir(termo_guarani, modelo=model, tokenizer=tokenizer, max_tentativas=3):\n","    \"\"\"\n","    Traduz termo do guarani para portugu√™s\n","    \"\"\"\n","    resultados = []\n","\n","    # Diferentes formatos de prompt\n","    prompts = [\n","        f\"Traduza do guarani para o portugu√™s: {termo_guarani} ->\",\n","        f\"Guarani: {termo_guarani} | Portugu√™s:\",\n","        f\"{termo_guarani} significa\"\n","    ]\n","\n","    modelo.eval()\n","    with torch.no_grad():\n","        for prompt in prompts:\n","            try:\n","                inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n","\n","                outputs = modelo.generate(\n","                    inputs.input_ids,\n","                    attention_mask=inputs.attention_mask,\n","                    max_length=inputs.input_ids.shape[1] + 20,\n","                    num_return_sequences=1,\n","                    temperature=0.7,\n","                    do_sample=True,\n","                    pad_token_id=tokenizer.eos_token_id,\n","                    no_repeat_ngram_size=2,\n","                    early_stopping=True\n","                )\n","\n","                resultado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","                # Extrair apenas a tradu√ß√£o\n","                if \"->\" in resultado:\n","                    traducao = resultado.split(\"->\")[-1].strip()\n","                elif \"|\" in resultado:\n","                    traducao = resultado.split(\"|\")[-1].replace(\"Portugu√™s:\", \"\").strip()\n","                else:\n","                    traducao = resultado.replace(prompt, \"\").strip()\n","\n","                if traducao and len(traducao) > 1:\n","                    resultados.append(traducao)\n","\n","            except Exception as e:\n","                print(f\"Erro no prompt '{prompt}': {e}\")\n","                continue\n","\n","    # Retornar a melhor tradu√ß√£o (mais comum ou primeira v√°lida)\n","    if resultados:\n","        return resultados[0]\n","    else:\n","        return \"Tradu√ß√£o n√£o encontrada\"\n","\n","# 19. Fun√ß√£o de avalia√ß√£o do modelo\n","def avaliar_modelo(exemplos_teste=None):\n","    \"\"\"\n","    Avalia o modelo com exemplos de teste\n","    \"\"\"\n","    if exemplos_teste is None:\n","        # Usar alguns exemplos dos dados de valida√ß√£o\n","        exemplos_teste = [\n","            (\"mba'e\", \"que/o que\"),\n","            (\"ava\", \"homem\"),\n","            (\"ku√±a\", \"mulher\"),\n","            (\"tembi'u\", \"comida\"),\n","            (\"y\", \"√°gua\")\n","        ]\n","\n","    print(\"=== AVALIA√á√ÉO DO MODELO ===\")\n","    acertos = 0\n","    total = len(exemplos_teste)\n","\n","    for guarani, esperado in exemplos_teste:\n","        traducao = traduzir(guarani)\n","        correto = any(palavra in traducao.lower() for palavra in esperado.lower().split('/'))\n","\n","        print(f\"Guarani: {guarani}\")\n","        print(f\"Esperado: {esperado}\")\n","        print(f\"Obtido: {traducao}\")\n","        print(f\"Correto: {'‚úì' if correto else '‚úó'}\")\n","        print(\"-\" * 40)\n","\n","        if correto:\n","            acertos += 1\n","\n","    precisao = (acertos / total) * 100\n","    print(f\"\\nPrecis√£o: {acertos}/{total} ({precisao:.1f}%)\")\n","    return precisao\n","\n","# 20. Executar testes\n","print(\"\\n\" + \"=\"*50)\n","print(\"TESTANDO MODELO TREINADO\")\n","print(\"=\"*50)\n","\n","# Exemplos de teste\n","exemplos_teste = [\n","    \"mba'e\",      # o que\n","    \"pe\",         # voc√™\n","    \"ha'e\",       # ele/ela\n","    \"ava\",        # homem\n","    \"tembi'u\",    # comida\n","]\n","\n","for exemplo in exemplos_teste:\n","    resultado = traduzir(exemplo)\n","    print(f\"'{exemplo}' -> '{resultado}'\")\n","\n","# Avaliar modelo se houver dados de teste\n","try:\n","    precisao = avaliar_modelo()\n","    print(f\"\\nModelo salvo com precis√£o de {precisao:.1f}%\")\n","except:\n","    print(\"\\nModelo salvo com sucesso!\")\n","\n","print(f\"\\nModelo final salvo em: {caminho_modelo_final}\")"],"metadata":{"id":"QQoJS1IIoAor","executionInfo":{"status":"aborted","timestamp":1748096508532,"user_tz":180,"elapsed":6979,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}}},"execution_count":null,"outputs":[]}]}