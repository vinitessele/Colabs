{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyP3LoabBSCW3+yB5jlOcdBf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jPfv7ZHluGSo"},"outputs":[],"source":["# 1. Instalar depend√™ncias\n","!pip install -q --upgrade transformers==4.30.2 datasets\n","\n","# 2. Verificar vers√£o\n","import torch\n","import transformers\n","print(f\"‚úÖ Vers√£o do Transformers: {transformers.__version__}\")\n","\n","# 3. Imports\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","\n","# 4. Carregar modelo e tokenizer\n","model_rubuntu = \"rubuntu/guarani-jopara-llm\"\n","tokenizer = AutoTokenizer.from_pretrained(model_rubuntu)\n","model = AutoModelForCausalLM.from_pretrained(model_rubuntu)\n","\n","# 5. Criar pipeline de gera√ß√£o com controle\n","translator = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    return_full_text=False  # s√≥ retorna a resposta\n",")\n","\n","# 6. Fun√ß√£o para formatar prompts\n","def format_prompt(instruction, input_text=\"\"):\n","    return f\"{instruction.strip()}\\n{input_text.strip()}\\n\"\n","\n","# 7. Casos de teste\n","test_cases = [\n","    (\"Traduza para o guarani:\", \"O rio est√° limpo\"),\n","    (\"Como se diz em Guarani:\", \"floresta\"),\n","    (\"Traduza para o portugu√™s:\", \"Yvyrareta\")\n","]\n","\n","# 8. Gera√ß√£o de tradu√ß√µes\n","for instruction, input_text in test_cases:\n","    prompt = format_prompt(instruction, input_text)\n","    result = translator(\n","        prompt,\n","        max_new_tokens=40,\n","        do_sample=True,\n","        temperature=0.7,\n","        top_p=0.9\n","    )\n","    print(f\"\\nüó£Ô∏è Prompt: {prompt.strip()}\")\n","    print(f\"üìù Tradu√ß√£o: {result[0]['generated_text']}\")"]}]}