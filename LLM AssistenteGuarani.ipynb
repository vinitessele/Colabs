{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMWI+dKPmuUBow0cy9GCqr6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"18527f6aab6940aa96149bbcbfabfa0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f76c3f25b86b44be99c07c84722ade09","IPY_MODEL_5729ad65253642e3baffee02137b3599","IPY_MODEL_dbd65ccc973d4543950f5003df93b5b6"],"layout":"IPY_MODEL_686259556ce04e54bfd4c460a5b0fc41"}},"f76c3f25b86b44be99c07c84722ade09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8339c8b3c11e4e57bbc11f286968e4b2","placeholder":"â€‹","style":"IPY_MODEL_7f41ac53723c41a49d6ffe88b477d13e","value":"Map:â€‡100%"}},"5729ad65253642e3baffee02137b3599":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb490d6c47484b3886d3f15e32efd6c8","max":6480,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab8ab152acbd424aa8776f47670a3d34","value":6480}},"dbd65ccc973d4543950f5003df93b5b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a135fc3df7bc451885c661a1963b5d63","placeholder":"â€‹","style":"IPY_MODEL_29dde41b594a4f2289e29198b3892d89","value":"â€‡6480/6480â€‡[00:03&lt;00:00,â€‡1791.74â€‡examples/s]"}},"686259556ce04e54bfd4c460a5b0fc41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8339c8b3c11e4e57bbc11f286968e4b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f41ac53723c41a49d6ffe88b477d13e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb490d6c47484b3886d3f15e32efd6c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab8ab152acbd424aa8776f47670a3d34":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a135fc3df7bc451885c661a1963b5d63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29dde41b594a4f2289e29198b3892d89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# ðŸ§  Treinamento de LLM - TraduÃ§Ã£o Guarani â†’ PortuguÃªs\n","Este notebook realiza o treinamento de uma LLM pequena (GPT-2) para traduÃ§Ã£o de palavras Guarani para o PortuguÃªs.\n","As etapas incluem: carregamento do dataset, anÃ¡lise estatÃ­stica, aumento de dados, tokenizaÃ§Ã£o, treinamento e testes."],"metadata":{"id":"l4TX9Tnu6BoE"}},{"cell_type":"code","source":["# âœ… ImportaÃ§Ãµes\n","import pandas as pd\n","import random\n","import json\n","import torch\n","from transformers import (\n","    GPT2LMHeadModel, GPT2Tokenizer,\n","    Trainer, TrainingArguments,\n","    DataCollatorForLanguageModeling\n",")\n","from datasets import Dataset\n","from google.colab import drive\n","# âœ… Carregar dados do Google Drive\n","def carregar_dados():\n","    drive.mount('/content/drive')\n","    df = pd.read_json(\"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/dados_treinamento_guarani.json\")\n","    print(f\"ðŸ“Š Total de exemplos no dataset: {len(df)}\")\n","    display(df.head())\n","    return df\n","\n","df = carregar_dados()\n","# âœ… EstatÃ­sticas\n","inputs = df['input'].tolist()\n","outputs = df['output'].tolist()\n","print(f\"ðŸ“ˆ Comprimento mÃ©dio dos inputs: {sum(len(i) for i in inputs)/len(inputs):.2f} caracteres\")\n","print(f\"ðŸ“ˆ Comprimento mÃ©dio dos outputs: {sum(len(o) for o in outputs)/len(outputs):.2f} caracteres\")\n","# âœ… Aumento de Dados\n","instructions_pt = [\n","    \"Traduza para o portuguÃªs:\", \"TraduÃ§Ã£o em portuguÃªs:\", \"O que significa em portuguÃªs:\",\n","    \"Traduza do Guarani para o PortuguÃªs:\", \"Como se diz em portuguÃªs:\",\"Converta para o portuguÃªs:\",\n","    \"Passe para o portuguÃªs:\",\"A traduÃ§Ã£o portuguesa Ã©:\"\n","]\n","instructions_gua = [\n","    \"Traduza para o guarani:\", \"VersÃ£o em Guarani:\", \"O que significa em Guarani:\",\n","    \"TraduÃ§Ã£o em Guarani:\", \"Como se diz em Guarani:\",\"Com respeito Ã  cultura Guarani\",\n","    \"Em guarani, significa:\",\n","]\n","\n","augmented_data = []\n","\n","for _, row in df.iterrows():\n","    for _ in range(5):\n","        inst = random.choice(instructions_pt)\n","        augmented_data.append({\n","            \"instruction\": inst,\n","            \"input\": row[\"input\"],\n","            \"output\": row[\"output\"]\n","        })\n","        inst_inv = random.choice(instructions_gua)\n","        augmented_data.append({\n","            \"instruction\": inst_inv,\n","            \"input\": row[\"output\"],\n","            \"output\": row[\"input\"]\n","        })\n","\n","print(f\"âœ… Dataset aumentado: {len(augmented_data)} exemplos\")\n","# âœ… TokenizaÃ§Ã£o\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"UZwWneY3QaVS","executionInfo":{"status":"ok","timestamp":1751162700786,"user_tz":180,"elapsed":2486,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"69d78556-2ac8-45b4-8383-859d56dec2c5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","ðŸ“Š Total de exemplos no dataset: 648\n"]},{"output_type":"display_data","data":{"text/plain":["                                  instruction             input         output\n","0    Como se diz 'filho (do pai)' em Guarani?    filho (do pai)        txeraâ€™y\n","1    Como se diz 'filha (da mÃ£e)' em Guarani?    filha (da mÃ£e)       txememby\n","2    Como se diz 'filha (do pai)' em Guarani?    filha (do pai)       txeradjy\n","3  Como se diz 'filho de criaÃ§Ã£o' em Guarani?  filho de criaÃ§Ã£o   txeraâ€™y rami\n","4  Como se diz 'filha de criaÃ§Ã£o' em Guarani?  filha de criaÃ§Ã£o  txeradjy rami"],"text/html":["\n","  <div id=\"df-495bd911-364e-411d-9f1f-175d5316a750\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>instruction</th>\n","      <th>input</th>\n","      <th>output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Como se diz 'filho (do pai)' em Guarani?</td>\n","      <td>filho (do pai)</td>\n","      <td>txeraâ€™y</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Como se diz 'filha (da mÃ£e)' em Guarani?</td>\n","      <td>filha (da mÃ£e)</td>\n","      <td>txememby</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Como se diz 'filha (do pai)' em Guarani?</td>\n","      <td>filha (do pai)</td>\n","      <td>txeradjy</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Como se diz 'filho de criaÃ§Ã£o' em Guarani?</td>\n","      <td>filho de criaÃ§Ã£o</td>\n","      <td>txeraâ€™y rami</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Como se diz 'filha de criaÃ§Ã£o' em Guarani?</td>\n","      <td>filha de criaÃ§Ã£o</td>\n","      <td>txeradjy rami</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-495bd911-364e-411d-9f1f-175d5316a750')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-495bd911-364e-411d-9f1f-175d5316a750 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-495bd911-364e-411d-9f1f-175d5316a750');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-9e6823b8-3fa9-4404-8ed6-57957788a385\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9e6823b8-3fa9-4404-8ed6-57957788a385')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-9e6823b8-3fa9-4404-8ed6-57957788a385 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"tokenizer\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"instruction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Como se diz 'filha (da m\\u00e3e)' em Guarani?\",\n          \"Como se diz 'filha de cria\\u00e7\\u00e3o' em Guarani?\",\n          \"Como se diz 'filha (do pai)' em Guarani?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"filha (da m\\u00e3e)\",\n          \"filha de cria\\u00e7\\u00e3o\",\n          \"filha (do pai)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"txememby\",\n          \"txeradjy rami\",\n          \"txeradjy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ðŸ“ˆ Comprimento mÃ©dio dos inputs: 13.13 caracteres\n","ðŸ“ˆ Comprimento mÃ©dio dos outputs: 16.94 caracteres\n","âœ… Dataset aumentado: 6480 exemplos\n"]}]},{"cell_type":"code","source":["\n","def format_and_tokenize(example):\n","    prompt = example[\"instruction\"] + \" \" + example[\"input\"]\n","    target = example[\"output\"]\n","    full_text = prompt + \" \" + target\n","    tokens = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=128)\n","    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n","    return tokens\n","\n","dataset = Dataset.from_list(augmented_data)\n","dataset_tokenized = dataset.map(format_and_tokenize)\n","# âœ… Treinamento\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","model.resize_token_embeddings(len(tokenizer))\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./guarani_model\",\n","    per_device_train_batch_size=8,\n","    num_train_epochs=5,\n","    logging_steps=10,\n","    save_steps=100,\n","    save_total_limit=1,\n","    logging_dir=\"./logs\",\n","    run_name=\"guarani_training_run\",  # Add unique run_name to avoid the warning\n","    report_to=\"none\"  # Disable W&B and other logging integrations\n",")\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset_tokenized,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n",")\n","\n","trainer.train()\n","trainer.save_model(\"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/guarani_gpt2\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["18527f6aab6940aa96149bbcbfabfa0e","f76c3f25b86b44be99c07c84722ade09","5729ad65253642e3baffee02137b3599","dbd65ccc973d4543950f5003df93b5b6","686259556ce04e54bfd4c460a5b0fc41","8339c8b3c11e4e57bbc11f286968e4b2","7f41ac53723c41a49d6ffe88b477d13e","fb490d6c47484b3886d3f15e32efd6c8","ab8ab152acbd424aa8776f47670a3d34","a135fc3df7bc451885c661a1963b5d63","29dde41b594a4f2289e29198b3892d89"]},"collapsed":true,"id":"7rhy96hflatD","executionInfo":{"status":"ok","timestamp":1751164532752,"user_tz":180,"elapsed":1831968,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"36e81f28-a87f-4b6c-c2e6-5805761d7a48"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/6480 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18527f6aab6940aa96149bbcbfabfa0e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-12-3313902332.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4050' max='4050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4050/4050 30:16, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>5.151000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>3.999900</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>3.836100</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>3.351100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>3.112300</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>2.954600</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>2.855800</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>2.856300</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>2.651500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>2.671700</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>2.597400</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>2.418900</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>2.601200</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>2.691100</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>2.386800</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>2.465800</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>2.270700</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>2.143800</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>2.194000</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>2.251900</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>2.280700</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>2.108800</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>2.295500</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>1.989100</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.989600</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>1.914300</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>1.847800</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>1.863700</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>2.007700</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>2.126500</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>1.931900</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>1.907600</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>1.952300</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>1.601600</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.663400</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>1.914000</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>1.853800</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>1.608000</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>1.563200</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.737100</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>1.498700</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>1.500600</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>1.579800</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>1.771000</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.630900</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>1.542000</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>1.623800</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>1.414900</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>1.645900</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.590000</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>1.581400</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>1.498300</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>1.441800</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>1.276600</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>1.271700</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>1.198600</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>1.314900</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>1.387600</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>1.318800</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.290200</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>1.351500</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>1.368000</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>1.163100</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>1.281200</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>1.422900</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>1.257400</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>1.146100</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>1.098800</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>1.139200</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.077300</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>1.101400</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>1.275000</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>1.089400</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>1.084700</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.092400</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>1.209700</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>1.033600</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>1.068100</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>1.111900</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.147400</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>1.276200</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>1.021600</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>0.967600</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>0.954200</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>0.949000</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>0.965200</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>0.940700</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>1.003900</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>0.991700</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.028600</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>0.874200</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>0.944200</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>0.887700</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>0.910500</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>0.839500</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>0.866500</td>\n","    </tr>\n","    <tr>\n","      <td>970</td>\n","      <td>0.908600</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>0.926500</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>0.859400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.769200</td>\n","    </tr>\n","    <tr>\n","      <td>1010</td>\n","      <td>0.805100</td>\n","    </tr>\n","    <tr>\n","      <td>1020</td>\n","      <td>0.817300</td>\n","    </tr>\n","    <tr>\n","      <td>1030</td>\n","      <td>0.828400</td>\n","    </tr>\n","    <tr>\n","      <td>1040</td>\n","      <td>0.851500</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>0.767000</td>\n","    </tr>\n","    <tr>\n","      <td>1060</td>\n","      <td>0.820600</td>\n","    </tr>\n","    <tr>\n","      <td>1070</td>\n","      <td>0.754600</td>\n","    </tr>\n","    <tr>\n","      <td>1080</td>\n","      <td>0.739500</td>\n","    </tr>\n","    <tr>\n","      <td>1090</td>\n","      <td>0.789300</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.720400</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>0.770200</td>\n","    </tr>\n","    <tr>\n","      <td>1120</td>\n","      <td>0.775500</td>\n","    </tr>\n","    <tr>\n","      <td>1130</td>\n","      <td>0.769400</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.806300</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>0.746700</td>\n","    </tr>\n","    <tr>\n","      <td>1160</td>\n","      <td>0.767800</td>\n","    </tr>\n","    <tr>\n","      <td>1170</td>\n","      <td>0.722700</td>\n","    </tr>\n","    <tr>\n","      <td>1180</td>\n","      <td>0.766800</td>\n","    </tr>\n","    <tr>\n","      <td>1190</td>\n","      <td>0.755600</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.736100</td>\n","    </tr>\n","    <tr>\n","      <td>1210</td>\n","      <td>0.749600</td>\n","    </tr>\n","    <tr>\n","      <td>1220</td>\n","      <td>0.705800</td>\n","    </tr>\n","    <tr>\n","      <td>1230</td>\n","      <td>0.775900</td>\n","    </tr>\n","    <tr>\n","      <td>1240</td>\n","      <td>0.736600</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.730200</td>\n","    </tr>\n","    <tr>\n","      <td>1260</td>\n","      <td>0.682900</td>\n","    </tr>\n","    <tr>\n","      <td>1270</td>\n","      <td>0.701600</td>\n","    </tr>\n","    <tr>\n","      <td>1280</td>\n","      <td>0.749300</td>\n","    </tr>\n","    <tr>\n","      <td>1290</td>\n","      <td>0.681800</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.775200</td>\n","    </tr>\n","    <tr>\n","      <td>1310</td>\n","      <td>0.731800</td>\n","    </tr>\n","    <tr>\n","      <td>1320</td>\n","      <td>0.699900</td>\n","    </tr>\n","    <tr>\n","      <td>1330</td>\n","      <td>0.725300</td>\n","    </tr>\n","    <tr>\n","      <td>1340</td>\n","      <td>0.670500</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>0.665100</td>\n","    </tr>\n","    <tr>\n","      <td>1360</td>\n","      <td>0.706800</td>\n","    </tr>\n","    <tr>\n","      <td>1370</td>\n","      <td>0.605900</td>\n","    </tr>\n","    <tr>\n","      <td>1380</td>\n","      <td>0.636000</td>\n","    </tr>\n","    <tr>\n","      <td>1390</td>\n","      <td>0.659600</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.660500</td>\n","    </tr>\n","    <tr>\n","      <td>1410</td>\n","      <td>0.624500</td>\n","    </tr>\n","    <tr>\n","      <td>1420</td>\n","      <td>0.646300</td>\n","    </tr>\n","    <tr>\n","      <td>1430</td>\n","      <td>0.597500</td>\n","    </tr>\n","    <tr>\n","      <td>1440</td>\n","      <td>0.623300</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>0.636800</td>\n","    </tr>\n","    <tr>\n","      <td>1460</td>\n","      <td>0.675900</td>\n","    </tr>\n","    <tr>\n","      <td>1470</td>\n","      <td>0.648500</td>\n","    </tr>\n","    <tr>\n","      <td>1480</td>\n","      <td>0.597100</td>\n","    </tr>\n","    <tr>\n","      <td>1490</td>\n","      <td>0.630500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.622800</td>\n","    </tr>\n","    <tr>\n","      <td>1510</td>\n","      <td>0.601700</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.617800</td>\n","    </tr>\n","    <tr>\n","      <td>1530</td>\n","      <td>0.586700</td>\n","    </tr>\n","    <tr>\n","      <td>1540</td>\n","      <td>0.593700</td>\n","    </tr>\n","    <tr>\n","      <td>1550</td>\n","      <td>0.654000</td>\n","    </tr>\n","    <tr>\n","      <td>1560</td>\n","      <td>0.647300</td>\n","    </tr>\n","    <tr>\n","      <td>1570</td>\n","      <td>0.651300</td>\n","    </tr>\n","    <tr>\n","      <td>1580</td>\n","      <td>0.636600</td>\n","    </tr>\n","    <tr>\n","      <td>1590</td>\n","      <td>0.607800</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.636900</td>\n","    </tr>\n","    <tr>\n","      <td>1610</td>\n","      <td>0.608000</td>\n","    </tr>\n","    <tr>\n","      <td>1620</td>\n","      <td>0.571700</td>\n","    </tr>\n","    <tr>\n","      <td>1630</td>\n","      <td>0.536800</td>\n","    </tr>\n","    <tr>\n","      <td>1640</td>\n","      <td>0.530400</td>\n","    </tr>\n","    <tr>\n","      <td>1650</td>\n","      <td>0.572100</td>\n","    </tr>\n","    <tr>\n","      <td>1660</td>\n","      <td>0.512100</td>\n","    </tr>\n","    <tr>\n","      <td>1670</td>\n","      <td>0.639100</td>\n","    </tr>\n","    <tr>\n","      <td>1680</td>\n","      <td>0.565000</td>\n","    </tr>\n","    <tr>\n","      <td>1690</td>\n","      <td>0.569000</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.534900</td>\n","    </tr>\n","    <tr>\n","      <td>1710</td>\n","      <td>0.559300</td>\n","    </tr>\n","    <tr>\n","      <td>1720</td>\n","      <td>0.560700</td>\n","    </tr>\n","    <tr>\n","      <td>1730</td>\n","      <td>0.495400</td>\n","    </tr>\n","    <tr>\n","      <td>1740</td>\n","      <td>0.551800</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.582100</td>\n","    </tr>\n","    <tr>\n","      <td>1760</td>\n","      <td>0.532400</td>\n","    </tr>\n","    <tr>\n","      <td>1770</td>\n","      <td>0.557200</td>\n","    </tr>\n","    <tr>\n","      <td>1780</td>\n","      <td>0.538900</td>\n","    </tr>\n","    <tr>\n","      <td>1790</td>\n","      <td>0.524200</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.559400</td>\n","    </tr>\n","    <tr>\n","      <td>1810</td>\n","      <td>0.524300</td>\n","    </tr>\n","    <tr>\n","      <td>1820</td>\n","      <td>0.538200</td>\n","    </tr>\n","    <tr>\n","      <td>1830</td>\n","      <td>0.559700</td>\n","    </tr>\n","    <tr>\n","      <td>1840</td>\n","      <td>0.563300</td>\n","    </tr>\n","    <tr>\n","      <td>1850</td>\n","      <td>0.529300</td>\n","    </tr>\n","    <tr>\n","      <td>1860</td>\n","      <td>0.530500</td>\n","    </tr>\n","    <tr>\n","      <td>1870</td>\n","      <td>0.508800</td>\n","    </tr>\n","    <tr>\n","      <td>1880</td>\n","      <td>0.533300</td>\n","    </tr>\n","    <tr>\n","      <td>1890</td>\n","      <td>0.510200</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.551600</td>\n","    </tr>\n","    <tr>\n","      <td>1910</td>\n","      <td>0.529100</td>\n","    </tr>\n","    <tr>\n","      <td>1920</td>\n","      <td>0.499000</td>\n","    </tr>\n","    <tr>\n","      <td>1930</td>\n","      <td>0.496900</td>\n","    </tr>\n","    <tr>\n","      <td>1940</td>\n","      <td>0.508000</td>\n","    </tr>\n","    <tr>\n","      <td>1950</td>\n","      <td>0.514600</td>\n","    </tr>\n","    <tr>\n","      <td>1960</td>\n","      <td>0.553200</td>\n","    </tr>\n","    <tr>\n","      <td>1970</td>\n","      <td>0.535100</td>\n","    </tr>\n","    <tr>\n","      <td>1980</td>\n","      <td>0.529500</td>\n","    </tr>\n","    <tr>\n","      <td>1990</td>\n","      <td>0.502300</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.536600</td>\n","    </tr>\n","    <tr>\n","      <td>2010</td>\n","      <td>0.498800</td>\n","    </tr>\n","    <tr>\n","      <td>2020</td>\n","      <td>0.532600</td>\n","    </tr>\n","    <tr>\n","      <td>2030</td>\n","      <td>0.473900</td>\n","    </tr>\n","    <tr>\n","      <td>2040</td>\n","      <td>0.446100</td>\n","    </tr>\n","    <tr>\n","      <td>2050</td>\n","      <td>0.547600</td>\n","    </tr>\n","    <tr>\n","      <td>2060</td>\n","      <td>0.530000</td>\n","    </tr>\n","    <tr>\n","      <td>2070</td>\n","      <td>0.494100</td>\n","    </tr>\n","    <tr>\n","      <td>2080</td>\n","      <td>0.452900</td>\n","    </tr>\n","    <tr>\n","      <td>2090</td>\n","      <td>0.476700</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.472100</td>\n","    </tr>\n","    <tr>\n","      <td>2110</td>\n","      <td>0.492200</td>\n","    </tr>\n","    <tr>\n","      <td>2120</td>\n","      <td>0.495900</td>\n","    </tr>\n","    <tr>\n","      <td>2130</td>\n","      <td>0.462900</td>\n","    </tr>\n","    <tr>\n","      <td>2140</td>\n","      <td>0.519600</td>\n","    </tr>\n","    <tr>\n","      <td>2150</td>\n","      <td>0.474800</td>\n","    </tr>\n","    <tr>\n","      <td>2160</td>\n","      <td>0.503700</td>\n","    </tr>\n","    <tr>\n","      <td>2170</td>\n","      <td>0.497800</td>\n","    </tr>\n","    <tr>\n","      <td>2180</td>\n","      <td>0.531900</td>\n","    </tr>\n","    <tr>\n","      <td>2190</td>\n","      <td>0.489200</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.460600</td>\n","    </tr>\n","    <tr>\n","      <td>2210</td>\n","      <td>0.439200</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>0.438800</td>\n","    </tr>\n","    <tr>\n","      <td>2230</td>\n","      <td>0.476800</td>\n","    </tr>\n","    <tr>\n","      <td>2240</td>\n","      <td>0.486700</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.477900</td>\n","    </tr>\n","    <tr>\n","      <td>2260</td>\n","      <td>0.479400</td>\n","    </tr>\n","    <tr>\n","      <td>2270</td>\n","      <td>0.489800</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.442800</td>\n","    </tr>\n","    <tr>\n","      <td>2290</td>\n","      <td>0.493100</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.479100</td>\n","    </tr>\n","    <tr>\n","      <td>2310</td>\n","      <td>0.454400</td>\n","    </tr>\n","    <tr>\n","      <td>2320</td>\n","      <td>0.425000</td>\n","    </tr>\n","    <tr>\n","      <td>2330</td>\n","      <td>0.459500</td>\n","    </tr>\n","    <tr>\n","      <td>2340</td>\n","      <td>0.467900</td>\n","    </tr>\n","    <tr>\n","      <td>2350</td>\n","      <td>0.491500</td>\n","    </tr>\n","    <tr>\n","      <td>2360</td>\n","      <td>0.419400</td>\n","    </tr>\n","    <tr>\n","      <td>2370</td>\n","      <td>0.474500</td>\n","    </tr>\n","    <tr>\n","      <td>2380</td>\n","      <td>0.441200</td>\n","    </tr>\n","    <tr>\n","      <td>2390</td>\n","      <td>0.500600</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.445600</td>\n","    </tr>\n","    <tr>\n","      <td>2410</td>\n","      <td>0.441600</td>\n","    </tr>\n","    <tr>\n","      <td>2420</td>\n","      <td>0.465100</td>\n","    </tr>\n","    <tr>\n","      <td>2430</td>\n","      <td>0.471100</td>\n","    </tr>\n","    <tr>\n","      <td>2440</td>\n","      <td>0.445000</td>\n","    </tr>\n","    <tr>\n","      <td>2450</td>\n","      <td>0.409600</td>\n","    </tr>\n","    <tr>\n","      <td>2460</td>\n","      <td>0.476500</td>\n","    </tr>\n","    <tr>\n","      <td>2470</td>\n","      <td>0.471800</td>\n","    </tr>\n","    <tr>\n","      <td>2480</td>\n","      <td>0.495400</td>\n","    </tr>\n","    <tr>\n","      <td>2490</td>\n","      <td>0.413900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.458700</td>\n","    </tr>\n","    <tr>\n","      <td>2510</td>\n","      <td>0.461900</td>\n","    </tr>\n","    <tr>\n","      <td>2520</td>\n","      <td>0.457500</td>\n","    </tr>\n","    <tr>\n","      <td>2530</td>\n","      <td>0.464300</td>\n","    </tr>\n","    <tr>\n","      <td>2540</td>\n","      <td>0.473100</td>\n","    </tr>\n","    <tr>\n","      <td>2550</td>\n","      <td>0.419200</td>\n","    </tr>\n","    <tr>\n","      <td>2560</td>\n","      <td>0.445800</td>\n","    </tr>\n","    <tr>\n","      <td>2570</td>\n","      <td>0.426700</td>\n","    </tr>\n","    <tr>\n","      <td>2580</td>\n","      <td>0.414500</td>\n","    </tr>\n","    <tr>\n","      <td>2590</td>\n","      <td>0.452500</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.404300</td>\n","    </tr>\n","    <tr>\n","      <td>2610</td>\n","      <td>0.426600</td>\n","    </tr>\n","    <tr>\n","      <td>2620</td>\n","      <td>0.433900</td>\n","    </tr>\n","    <tr>\n","      <td>2630</td>\n","      <td>0.496700</td>\n","    </tr>\n","    <tr>\n","      <td>2640</td>\n","      <td>0.436500</td>\n","    </tr>\n","    <tr>\n","      <td>2650</td>\n","      <td>0.446700</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.464400</td>\n","    </tr>\n","    <tr>\n","      <td>2670</td>\n","      <td>0.454300</td>\n","    </tr>\n","    <tr>\n","      <td>2680</td>\n","      <td>0.423400</td>\n","    </tr>\n","    <tr>\n","      <td>2690</td>\n","      <td>0.442300</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.467600</td>\n","    </tr>\n","    <tr>\n","      <td>2710</td>\n","      <td>0.432000</td>\n","    </tr>\n","    <tr>\n","      <td>2720</td>\n","      <td>0.441300</td>\n","    </tr>\n","    <tr>\n","      <td>2730</td>\n","      <td>0.404500</td>\n","    </tr>\n","    <tr>\n","      <td>2740</td>\n","      <td>0.419100</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>0.446500</td>\n","    </tr>\n","    <tr>\n","      <td>2760</td>\n","      <td>0.410000</td>\n","    </tr>\n","    <tr>\n","      <td>2770</td>\n","      <td>0.401100</td>\n","    </tr>\n","    <tr>\n","      <td>2780</td>\n","      <td>0.428800</td>\n","    </tr>\n","    <tr>\n","      <td>2790</td>\n","      <td>0.411600</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.456900</td>\n","    </tr>\n","    <tr>\n","      <td>2810</td>\n","      <td>0.451500</td>\n","    </tr>\n","    <tr>\n","      <td>2820</td>\n","      <td>0.414700</td>\n","    </tr>\n","    <tr>\n","      <td>2830</td>\n","      <td>0.472800</td>\n","    </tr>\n","    <tr>\n","      <td>2840</td>\n","      <td>0.427700</td>\n","    </tr>\n","    <tr>\n","      <td>2850</td>\n","      <td>0.454000</td>\n","    </tr>\n","    <tr>\n","      <td>2860</td>\n","      <td>0.429000</td>\n","    </tr>\n","    <tr>\n","      <td>2870</td>\n","      <td>0.407200</td>\n","    </tr>\n","    <tr>\n","      <td>2880</td>\n","      <td>0.476900</td>\n","    </tr>\n","    <tr>\n","      <td>2890</td>\n","      <td>0.469100</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.436100</td>\n","    </tr>\n","    <tr>\n","      <td>2910</td>\n","      <td>0.454700</td>\n","    </tr>\n","    <tr>\n","      <td>2920</td>\n","      <td>0.422400</td>\n","    </tr>\n","    <tr>\n","      <td>2930</td>\n","      <td>0.386300</td>\n","    </tr>\n","    <tr>\n","      <td>2940</td>\n","      <td>0.456100</td>\n","    </tr>\n","    <tr>\n","      <td>2950</td>\n","      <td>0.455600</td>\n","    </tr>\n","    <tr>\n","      <td>2960</td>\n","      <td>0.403700</td>\n","    </tr>\n","    <tr>\n","      <td>2970</td>\n","      <td>0.439100</td>\n","    </tr>\n","    <tr>\n","      <td>2980</td>\n","      <td>0.404200</td>\n","    </tr>\n","    <tr>\n","      <td>2990</td>\n","      <td>0.395500</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.441000</td>\n","    </tr>\n","    <tr>\n","      <td>3010</td>\n","      <td>0.401600</td>\n","    </tr>\n","    <tr>\n","      <td>3020</td>\n","      <td>0.434400</td>\n","    </tr>\n","    <tr>\n","      <td>3030</td>\n","      <td>0.370100</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.445800</td>\n","    </tr>\n","    <tr>\n","      <td>3050</td>\n","      <td>0.441700</td>\n","    </tr>\n","    <tr>\n","      <td>3060</td>\n","      <td>0.465800</td>\n","    </tr>\n","    <tr>\n","      <td>3070</td>\n","      <td>0.424900</td>\n","    </tr>\n","    <tr>\n","      <td>3080</td>\n","      <td>0.443900</td>\n","    </tr>\n","    <tr>\n","      <td>3090</td>\n","      <td>0.440000</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.420800</td>\n","    </tr>\n","    <tr>\n","      <td>3110</td>\n","      <td>0.449200</td>\n","    </tr>\n","    <tr>\n","      <td>3120</td>\n","      <td>0.437100</td>\n","    </tr>\n","    <tr>\n","      <td>3130</td>\n","      <td>0.415700</td>\n","    </tr>\n","    <tr>\n","      <td>3140</td>\n","      <td>0.410800</td>\n","    </tr>\n","    <tr>\n","      <td>3150</td>\n","      <td>0.418500</td>\n","    </tr>\n","    <tr>\n","      <td>3160</td>\n","      <td>0.381300</td>\n","    </tr>\n","    <tr>\n","      <td>3170</td>\n","      <td>0.404500</td>\n","    </tr>\n","    <tr>\n","      <td>3180</td>\n","      <td>0.429000</td>\n","    </tr>\n","    <tr>\n","      <td>3190</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.435800</td>\n","    </tr>\n","    <tr>\n","      <td>3210</td>\n","      <td>0.393300</td>\n","    </tr>\n","    <tr>\n","      <td>3220</td>\n","      <td>0.440000</td>\n","    </tr>\n","    <tr>\n","      <td>3230</td>\n","      <td>0.398900</td>\n","    </tr>\n","    <tr>\n","      <td>3240</td>\n","      <td>0.446500</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>0.424400</td>\n","    </tr>\n","    <tr>\n","      <td>3260</td>\n","      <td>0.381800</td>\n","    </tr>\n","    <tr>\n","      <td>3270</td>\n","      <td>0.387900</td>\n","    </tr>\n","    <tr>\n","      <td>3280</td>\n","      <td>0.417900</td>\n","    </tr>\n","    <tr>\n","      <td>3290</td>\n","      <td>0.436400</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.407100</td>\n","    </tr>\n","    <tr>\n","      <td>3310</td>\n","      <td>0.406200</td>\n","    </tr>\n","    <tr>\n","      <td>3320</td>\n","      <td>0.409500</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>0.389600</td>\n","    </tr>\n","    <tr>\n","      <td>3340</td>\n","      <td>0.415000</td>\n","    </tr>\n","    <tr>\n","      <td>3350</td>\n","      <td>0.407500</td>\n","    </tr>\n","    <tr>\n","      <td>3360</td>\n","      <td>0.439900</td>\n","    </tr>\n","    <tr>\n","      <td>3370</td>\n","      <td>0.417600</td>\n","    </tr>\n","    <tr>\n","      <td>3380</td>\n","      <td>0.414100</td>\n","    </tr>\n","    <tr>\n","      <td>3390</td>\n","      <td>0.383200</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.385500</td>\n","    </tr>\n","    <tr>\n","      <td>3410</td>\n","      <td>0.399500</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.390300</td>\n","    </tr>\n","    <tr>\n","      <td>3430</td>\n","      <td>0.391300</td>\n","    </tr>\n","    <tr>\n","      <td>3440</td>\n","      <td>0.419100</td>\n","    </tr>\n","    <tr>\n","      <td>3450</td>\n","      <td>0.444900</td>\n","    </tr>\n","    <tr>\n","      <td>3460</td>\n","      <td>0.390700</td>\n","    </tr>\n","    <tr>\n","      <td>3470</td>\n","      <td>0.412100</td>\n","    </tr>\n","    <tr>\n","      <td>3480</td>\n","      <td>0.411300</td>\n","    </tr>\n","    <tr>\n","      <td>3490</td>\n","      <td>0.436600</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.397300</td>\n","    </tr>\n","    <tr>\n","      <td>3510</td>\n","      <td>0.422100</td>\n","    </tr>\n","    <tr>\n","      <td>3520</td>\n","      <td>0.393200</td>\n","    </tr>\n","    <tr>\n","      <td>3530</td>\n","      <td>0.396600</td>\n","    </tr>\n","    <tr>\n","      <td>3540</td>\n","      <td>0.381800</td>\n","    </tr>\n","    <tr>\n","      <td>3550</td>\n","      <td>0.405100</td>\n","    </tr>\n","    <tr>\n","      <td>3560</td>\n","      <td>0.366100</td>\n","    </tr>\n","    <tr>\n","      <td>3570</td>\n","      <td>0.380000</td>\n","    </tr>\n","    <tr>\n","      <td>3580</td>\n","      <td>0.395300</td>\n","    </tr>\n","    <tr>\n","      <td>3590</td>\n","      <td>0.383900</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.427300</td>\n","    </tr>\n","    <tr>\n","      <td>3610</td>\n","      <td>0.328700</td>\n","    </tr>\n","    <tr>\n","      <td>3620</td>\n","      <td>0.410200</td>\n","    </tr>\n","    <tr>\n","      <td>3630</td>\n","      <td>0.399600</td>\n","    </tr>\n","    <tr>\n","      <td>3640</td>\n","      <td>0.391800</td>\n","    </tr>\n","    <tr>\n","      <td>3650</td>\n","      <td>0.402800</td>\n","    </tr>\n","    <tr>\n","      <td>3660</td>\n","      <td>0.424900</td>\n","    </tr>\n","    <tr>\n","      <td>3670</td>\n","      <td>0.432600</td>\n","    </tr>\n","    <tr>\n","      <td>3680</td>\n","      <td>0.373400</td>\n","    </tr>\n","    <tr>\n","      <td>3690</td>\n","      <td>0.384600</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.390000</td>\n","    </tr>\n","    <tr>\n","      <td>3710</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3720</td>\n","      <td>0.409700</td>\n","    </tr>\n","    <tr>\n","      <td>3730</td>\n","      <td>0.439800</td>\n","    </tr>\n","    <tr>\n","      <td>3740</td>\n","      <td>0.398600</td>\n","    </tr>\n","    <tr>\n","      <td>3750</td>\n","      <td>0.421600</td>\n","    </tr>\n","    <tr>\n","      <td>3760</td>\n","      <td>0.398000</td>\n","    </tr>\n","    <tr>\n","      <td>3770</td>\n","      <td>0.381000</td>\n","    </tr>\n","    <tr>\n","      <td>3780</td>\n","      <td>0.351800</td>\n","    </tr>\n","    <tr>\n","      <td>3790</td>\n","      <td>0.406300</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.397700</td>\n","    </tr>\n","    <tr>\n","      <td>3810</td>\n","      <td>0.383700</td>\n","    </tr>\n","    <tr>\n","      <td>3820</td>\n","      <td>0.417100</td>\n","    </tr>\n","    <tr>\n","      <td>3830</td>\n","      <td>0.392100</td>\n","    </tr>\n","    <tr>\n","      <td>3840</td>\n","      <td>0.409100</td>\n","    </tr>\n","    <tr>\n","      <td>3850</td>\n","      <td>0.386400</td>\n","    </tr>\n","    <tr>\n","      <td>3860</td>\n","      <td>0.400300</td>\n","    </tr>\n","    <tr>\n","      <td>3870</td>\n","      <td>0.396500</td>\n","    </tr>\n","    <tr>\n","      <td>3880</td>\n","      <td>0.395000</td>\n","    </tr>\n","    <tr>\n","      <td>3890</td>\n","      <td>0.434400</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>0.405100</td>\n","    </tr>\n","    <tr>\n","      <td>3910</td>\n","      <td>0.440000</td>\n","    </tr>\n","    <tr>\n","      <td>3920</td>\n","      <td>0.389300</td>\n","    </tr>\n","    <tr>\n","      <td>3930</td>\n","      <td>0.396600</td>\n","    </tr>\n","    <tr>\n","      <td>3940</td>\n","      <td>0.397800</td>\n","    </tr>\n","    <tr>\n","      <td>3950</td>\n","      <td>0.408700</td>\n","    </tr>\n","    <tr>\n","      <td>3960</td>\n","      <td>0.384800</td>\n","    </tr>\n","    <tr>\n","      <td>3970</td>\n","      <td>0.422000</td>\n","    </tr>\n","    <tr>\n","      <td>3980</td>\n","      <td>0.392300</td>\n","    </tr>\n","    <tr>\n","      <td>3990</td>\n","      <td>0.455300</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.381800</td>\n","    </tr>\n","    <tr>\n","      <td>4010</td>\n","      <td>0.416000</td>\n","    </tr>\n","    <tr>\n","      <td>4020</td>\n","      <td>0.371400</td>\n","    </tr>\n","    <tr>\n","      <td>4030</td>\n","      <td>0.409400</td>\n","    </tr>\n","    <tr>\n","      <td>4040</td>\n","      <td>0.384100</td>\n","    </tr>\n","    <tr>\n","      <td>4050</td>\n","      <td>0.399900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["# âœ… Testes do modelo\n","def testar_modelo(prompt_input):\n","    input_text = \"Traduza para o portuguÃªs: \" + prompt_input\n","    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n","    output = model.generate(input_ids, max_new_tokens=30, do_sample=True)\n","    print(f\"> Input: {prompt_input}\")\n","    print(\"> TraduÃ§Ã£o:\", tokenizer.decode(output[0], skip_special_tokens=True))\n","\n","# Exemplos de teste\n","testar_modelo(\"Djety-mbowÃ©\")\n","testar_modelo(\"Kamby\")\n","testar_modelo(\"EÃ­\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3kPz6Le9mVH","executionInfo":{"status":"ok","timestamp":1751164533818,"user_tz":180,"elapsed":1063,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"23546ad9-3df3-4522-f539-75c54949a8cf"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["> Input: Djety-mbowÃ©\n","> TraduÃ§Ã£o: Traduza para o portuguÃªs: Djety-mbowÃ© banha! Banha ky-re'áº½! Vai com Deus PetapeÃ³ larva do che! Vai com Deus\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["> Input: Kamby\n","> TraduÃ§Ã£o: Traduza para o portuguÃªs: Kamby leite Kamby leite Como viviam Nhaningarekhe'áº½. Como viviam Nhe'\n","> Input: EÃ­\n","> TraduÃ§Ã£o: Traduza para o portuguÃªs: EÃ­ mel de abelhas atÃ©. Vicho animal aldeia Nossa flor Ã© muito bela. Upegwi adj\n"]}]},{"cell_type":"code","source":["#!pip install evaluate sacrebleu\n","import evaluate\n","import numpy as np\n","\n","# Carregar mÃ©tricas\n","bleu = evaluate.load(\"sacrebleu\")\n","\n","# Gerar previsÃµes para o conjunto de teste\n","samples = df.sample(50, random_state=42)  # ou usar parte de `augmented_data`\n","\n","referencias = []\n","predicoes = []\n","\n","for _, row in samples.iterrows():\n","    entrada = row[\"input\"]\n","    referencia = row[\"output\"]\n","\n","    input_text = \"Traduza para o portuguÃªs: \" + entrada\n","    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n","    output = model.generate(input_ids, max_new_tokens=30, do_sample=False)\n","    saida = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","    # PÃ³s-processamento para pegar sÃ³ a traduÃ§Ã£o\n","    traducao = saida.replace(input_text, \"\").strip()\n","\n","    referencias.append([referencia])\n","    predicoes.append(traducao)\n","\n","# Calcular BLEU\n","bleu_result = bleu.compute(predictions=predicoes, references=referencias)\n","print(f\"ðŸ“˜ BLEU score: {bleu_result['score']:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iFC9HWxUzXVT","executionInfo":{"status":"ok","timestamp":1751164555559,"user_tz":180,"elapsed":21733,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"652d6979-d837-4cc1-a2c0-c09efc554c5e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["ðŸ“˜ BLEU score: 7.86\n"]}]},{"cell_type":"code","source":["from torch.nn import CrossEntropyLoss\n","\n","def calcular_perplexity(model, dataset_tokenized, n=100):\n","    model.eval()\n","    losses = []\n","    # converte para dicionÃ¡rio com listas para cada chave\n","    batches = dataset_tokenized.select(range(n)).to_dict()\n","    for i in range(n):\n","        input_ids = batches['input_ids'][i]\n","        labels = batches['labels'][i]\n","        inputs = torch.tensor([input_ids]).to(model.device)\n","        labels = torch.tensor([labels]).to(model.device)\n","        with torch.no_grad():\n","            outputs = model(inputs, labels=labels)\n","        losses.append(outputs.loss.item())\n","    mean_loss = np.mean(losses)\n","    perplexity = np.exp(mean_loss)\n","    print(f\"ðŸ”¢ Perplexity: {perplexity:.2f}\")\n","    return perplexity\n","\n","# Chamada:\n","calcular_perplexity(model, dataset_tokenized)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"siSObFRhzN8g","executionInfo":{"status":"ok","timestamp":1751164557962,"user_tz":180,"elapsed":2400,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"6c0c8a79-17f9-4a8e-a927-b4b9a32c0c2b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ”¢ Perplexity: 77259878116.51\n"]},{"output_type":"execute_result","data":{"text/plain":["np.float64(77259878116.50635)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","\n","# âœ… Caminho do modelo salvo no seu Google Drive\n","caminho_modelo = \"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/guarani_gpt2\"\n","\n","# âœ… Carregar modelo e tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(caminho_modelo)\n","model = GPT2LMHeadModel.from_pretrained(caminho_modelo)\n","model.eval()\n","\n","# âœ… Enviar modelo para GPU se disponÃ­vel\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# âœ… FunÃ§Ã£o de traduÃ§Ã£o\n","attention_mask = torch.ones_like(input_ids)\n","# Configurar pad_token\n","tokenizer.pad_token = tokenizer.eos_token\n","model.config.pad_token_id = tokenizer.eos_token_id\n","\n","def traduzir_guarani(texto_guarani):\n","    prompt = \"Traduza para o portuguÃªs: \" + texto_guarani\n","    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n","    output = model.generate(input_ids,\n","                            attention_mask=attention_mask,\n","                            max_length=50,\n","                            num_beams=4,\n","                            max_new_tokens=30,\n","                            do_sample=True,\n","                            early_stopping=True,\n","                            no_repeat_ngram_size=2,\n","                            temperature=0.5)\n","    traducao = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return traducao\n","\n","# âœ… Loop interativo\n","print(\"ðŸ”¡ Digite uma palavra/frase em Guarani (ou 'sair' para encerrar):\")\n","while True:\n","    entrada = input(\"ðŸ“ Guarani: \")\n","    if entrada.lower() in [\"sair\", \"exit\", \"quit\"]:\n","        print(\"ðŸ‘‹ Encerrando.\")\n","        break\n","    resposta = traduzir_guarani(entrada)\n","    print(\"ðŸ“˜ TraduÃ§Ã£o:\", resposta)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"id":"W0PZUlmBaPnD","executionInfo":{"status":"error","timestamp":1751164797606,"user_tz":180,"elapsed":168980,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"0b667f41-976c-4c3d-c3ae-fee2b55a9e3b"},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":["ðŸ”¡ Digite uma palavra/frase em Guarani (ou 'sair' para encerrar):\n","ðŸ“ Guarani: menino\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Both `max_new_tokens` (=30) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["ðŸ“˜ TraduÃ§Ã£o: Traduza para o portuguÃªs: menino nandadadurururanuraniruraniruiruirirunir uniriru uniru uiru Uiru Uniru Unioniru Universal\n","ðŸ“ Guarani: Bom dia\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Both `max_new_tokens` (=30) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["ðŸ“˜ TraduÃ§Ã£o: Traduza para o portuguÃªs: Bom dia j jjjajajjawajawawadawadaawdawbyawandawwereandandwereadandareandad\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-18-468140770.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸ”¡ Digite uma palavra/frase em Guarani (ou 'sair' para encerrar):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mentrada\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸ“ Guarani: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mentrada\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"sair\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸ‘‹ Encerrando.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":["def traduzir_corrigido(entrada):\n","    input_text = f\"Traduza para o portuguÃªs: {entrada}\"\n","    inputs = tokenizer(input_text, return_tensors=\"pt\")\n","\n","    output = model.generate(\n","        inputs.input_ids,\n","        attention_mask=inputs.attention_mask,\n","        max_new_tokens=15,\n","        temperature=1.0,\n","        repetition_penalty=1.5,\n","        no_repeat_ngram_size=2,\n","        do_sample=True,\n","        top_p=0.8\n","    )\n","\n","    result = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return result.replace(input_text, \"\").strip()\n","\n","# Teste\n","print(traduzir_corrigido(\"Kamby\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"jNtDgT3hjf8y","executionInfo":{"status":"error","timestamp":1751164800808,"user_tz":180,"elapsed":150,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"90d4fb7a-10b0-4f58-a2f0-506fdea3bafc"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-19-2390786966.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraduzir_corrigido\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Kamby\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-19-2390786966.py\u001b[0m in \u001b[0;36mtraduzir_corrigido\u001b[0;34m(entrada)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     output = model.generate(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2598\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3557\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3558\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1210\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1211\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcache_position\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"]}]}]}