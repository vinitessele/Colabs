{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMWI+dKPmuUBow0cy9GCqr6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"18527f6aab6940aa96149bbcbfabfa0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f76c3f25b86b44be99c07c84722ade09","IPY_MODEL_5729ad65253642e3baffee02137b3599","IPY_MODEL_dbd65ccc973d4543950f5003df93b5b6"],"layout":"IPY_MODEL_686259556ce04e54bfd4c460a5b0fc41"}},"f76c3f25b86b44be99c07c84722ade09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8339c8b3c11e4e57bbc11f286968e4b2","placeholder":"​","style":"IPY_MODEL_7f41ac53723c41a49d6ffe88b477d13e","value":"Map: 100%"}},"5729ad65253642e3baffee02137b3599":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb490d6c47484b3886d3f15e32efd6c8","max":6480,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab8ab152acbd424aa8776f47670a3d34","value":6480}},"dbd65ccc973d4543950f5003df93b5b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a135fc3df7bc451885c661a1963b5d63","placeholder":"​","style":"IPY_MODEL_29dde41b594a4f2289e29198b3892d89","value":" 6480/6480 [00:03&lt;00:00, 1791.74 examples/s]"}},"686259556ce04e54bfd4c460a5b0fc41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8339c8b3c11e4e57bbc11f286968e4b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f41ac53723c41a49d6ffe88b477d13e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb490d6c47484b3886d3f15e32efd6c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab8ab152acbd424aa8776f47670a3d34":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a135fc3df7bc451885c661a1963b5d63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29dde41b594a4f2289e29198b3892d89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# 🧠 Treinamento de LLM - Tradução Guarani → Português\n","Este notebook realiza o treinamento de uma LLM pequena (GPT-2) para tradução de palavras Guarani para o Português.\n","As etapas incluem: carregamento do dataset, análise estatística, aumento de dados, tokenização, treinamento e testes."],"metadata":{"id":"l4TX9Tnu6BoE"}},{"cell_type":"code","source":["# ✅ Importações\n","import pandas as pd\n","import random\n","import json\n","import torch\n","from transformers import (\n","    GPT2LMHeadModel, GPT2Tokenizer,\n","    Trainer, TrainingArguments,\n","    DataCollatorForLanguageModeling\n",")\n","from datasets import Dataset\n","from google.colab import drive\n","# ✅ Carregar dados do Google Drive\n","def carregar_dados():\n","    drive.mount('/content/drive')\n","    df = pd.read_json(\"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/dados_treinamento_guarani.json\")\n","    print(f\"📊 Total de exemplos no dataset: {len(df)}\")\n","    display(df.head())\n","    return df\n","\n","df = carregar_dados()\n","# ✅ Estatísticas\n","inputs = df['input'].tolist()\n","outputs = df['output'].tolist()\n","print(f\"📈 Comprimento médio dos inputs: {sum(len(i) for i in inputs)/len(inputs):.2f} caracteres\")\n","print(f\"📈 Comprimento médio dos outputs: {sum(len(o) for o in outputs)/len(outputs):.2f} caracteres\")\n","# ✅ Aumento de Dados\n","instructions_pt = [\n","    \"Traduza para o português:\", \"Tradução em português:\", \"O que significa em português:\",\n","    \"Traduza do Guarani para o Português:\", \"Como se diz em português:\",\"Converta para o português:\",\n","    \"Passe para o português:\",\"A tradução portuguesa é:\"\n","]\n","instructions_gua = [\n","    \"Traduza para o guarani:\", \"Versão em Guarani:\", \"O que significa em Guarani:\",\n","    \"Tradução em Guarani:\", \"Como se diz em Guarani:\",\"Com respeito à cultura Guarani\",\n","    \"Em guarani, significa:\",\n","]\n","\n","augmented_data = []\n","\n","for _, row in df.iterrows():\n","    for _ in range(5):\n","        inst = random.choice(instructions_pt)\n","        augmented_data.append({\n","            \"instruction\": inst,\n","            \"input\": row[\"input\"],\n","            \"output\": row[\"output\"]\n","        })\n","        inst_inv = random.choice(instructions_gua)\n","        augmented_data.append({\n","            \"instruction\": inst_inv,\n","            \"input\": row[\"output\"],\n","            \"output\": row[\"input\"]\n","        })\n","\n","print(f\"✅ Dataset aumentado: {len(augmented_data)} exemplos\")\n","# ✅ Tokenização\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"UZwWneY3QaVS","executionInfo":{"status":"ok","timestamp":1751162700786,"user_tz":180,"elapsed":2486,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"69d78556-2ac8-45b4-8383-859d56dec2c5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","📊 Total de exemplos no dataset: 648\n"]},{"output_type":"display_data","data":{"text/plain":["                                  instruction             input         output\n","0    Como se diz 'filho (do pai)' em Guarani?    filho (do pai)        txera’y\n","1    Como se diz 'filha (da mãe)' em Guarani?    filha (da mãe)       txememby\n","2    Como se diz 'filha (do pai)' em Guarani?    filha (do pai)       txeradjy\n","3  Como se diz 'filho de criação' em Guarani?  filho de criação   txera’y rami\n","4  Como se diz 'filha de criação' em Guarani?  filha de criação  txeradjy rami"],"text/html":["\n","  <div id=\"df-495bd911-364e-411d-9f1f-175d5316a750\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>instruction</th>\n","      <th>input</th>\n","      <th>output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Como se diz 'filho (do pai)' em Guarani?</td>\n","      <td>filho (do pai)</td>\n","      <td>txera’y</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Como se diz 'filha (da mãe)' em Guarani?</td>\n","      <td>filha (da mãe)</td>\n","      <td>txememby</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Como se diz 'filha (do pai)' em Guarani?</td>\n","      <td>filha (do pai)</td>\n","      <td>txeradjy</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Como se diz 'filho de criação' em Guarani?</td>\n","      <td>filho de criação</td>\n","      <td>txera’y rami</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Como se diz 'filha de criação' em Guarani?</td>\n","      <td>filha de criação</td>\n","      <td>txeradjy rami</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-495bd911-364e-411d-9f1f-175d5316a750')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-495bd911-364e-411d-9f1f-175d5316a750 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-495bd911-364e-411d-9f1f-175d5316a750');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-9e6823b8-3fa9-4404-8ed6-57957788a385\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9e6823b8-3fa9-4404-8ed6-57957788a385')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-9e6823b8-3fa9-4404-8ed6-57957788a385 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"tokenizer\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"instruction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Como se diz 'filha (da m\\u00e3e)' em Guarani?\",\n          \"Como se diz 'filha de cria\\u00e7\\u00e3o' em Guarani?\",\n          \"Como se diz 'filha (do pai)' em Guarani?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"filha (da m\\u00e3e)\",\n          \"filha de cria\\u00e7\\u00e3o\",\n          \"filha (do pai)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"txememby\",\n          \"txeradjy rami\",\n          \"txeradjy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["📈 Comprimento médio dos inputs: 13.13 caracteres\n","📈 Comprimento médio dos outputs: 16.94 caracteres\n","✅ Dataset aumentado: 6480 exemplos\n"]}]},{"cell_type":"code","source":["\n","def format_and_tokenize(example):\n","    prompt = example[\"instruction\"] + \" \" + example[\"input\"]\n","    target = example[\"output\"]\n","    full_text = prompt + \" \" + target\n","    tokens = tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=128)\n","    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n","    return tokens\n","\n","dataset = Dataset.from_list(augmented_data)\n","dataset_tokenized = dataset.map(format_and_tokenize)\n","# ✅ Treinamento\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","model.resize_token_embeddings(len(tokenizer))\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./guarani_model\",\n","    per_device_train_batch_size=8,\n","    num_train_epochs=5,\n","    logging_steps=10,\n","    save_steps=100,\n","    save_total_limit=1,\n","    logging_dir=\"./logs\",\n","    run_name=\"guarani_training_run\",  # Add unique run_name to avoid the warning\n","    report_to=\"none\"  # Disable W&B and other logging integrations\n",")\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset_tokenized,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n",")\n","\n","trainer.train()\n","trainer.save_model(\"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/guarani_gpt2\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["18527f6aab6940aa96149bbcbfabfa0e","f76c3f25b86b44be99c07c84722ade09","5729ad65253642e3baffee02137b3599","dbd65ccc973d4543950f5003df93b5b6","686259556ce04e54bfd4c460a5b0fc41","8339c8b3c11e4e57bbc11f286968e4b2","7f41ac53723c41a49d6ffe88b477d13e","fb490d6c47484b3886d3f15e32efd6c8","ab8ab152acbd424aa8776f47670a3d34","a135fc3df7bc451885c661a1963b5d63","29dde41b594a4f2289e29198b3892d89"]},"collapsed":true,"id":"7rhy96hflatD","executionInfo":{"status":"ok","timestamp":1751164532752,"user_tz":180,"elapsed":1831968,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"36e81f28-a87f-4b6c-c2e6-5805761d7a48"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/6480 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18527f6aab6940aa96149bbcbfabfa0e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-12-3313902332.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4050' max='4050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4050/4050 30:16, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>5.151000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>3.999900</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>3.836100</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>3.351100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>3.112300</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>2.954600</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>2.855800</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>2.856300</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>2.651500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>2.671700</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>2.597400</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>2.418900</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>2.601200</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>2.691100</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>2.386800</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>2.465800</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>2.270700</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>2.143800</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>2.194000</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>2.251900</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>2.280700</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>2.108800</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>2.295500</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>1.989100</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.989600</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>1.914300</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>1.847800</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>1.863700</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>2.007700</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>2.126500</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>1.931900</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>1.907600</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>1.952300</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>1.601600</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.663400</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>1.914000</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>1.853800</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>1.608000</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>1.563200</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.737100</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>1.498700</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>1.500600</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>1.579800</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>1.771000</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.630900</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>1.542000</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>1.623800</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>1.414900</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>1.645900</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.590000</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>1.581400</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>1.498300</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>1.441800</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>1.276600</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>1.271700</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>1.198600</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>1.314900</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>1.387600</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>1.318800</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.290200</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>1.351500</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>1.368000</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>1.163100</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>1.281200</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>1.422900</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>1.257400</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>1.146100</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>1.098800</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>1.139200</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.077300</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>1.101400</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>1.275000</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>1.089400</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>1.084700</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.092400</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>1.209700</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>1.033600</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>1.068100</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>1.111900</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.147400</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>1.276200</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>1.021600</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>0.967600</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>0.954200</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>0.949000</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>0.965200</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>0.940700</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>1.003900</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>0.991700</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.028600</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>0.874200</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>0.944200</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>0.887700</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>0.910500</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>0.839500</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>0.866500</td>\n","    </tr>\n","    <tr>\n","      <td>970</td>\n","      <td>0.908600</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>0.926500</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>0.859400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.769200</td>\n","    </tr>\n","    <tr>\n","      <td>1010</td>\n","      <td>0.805100</td>\n","    </tr>\n","    <tr>\n","      <td>1020</td>\n","      <td>0.817300</td>\n","    </tr>\n","    <tr>\n","      <td>1030</td>\n","      <td>0.828400</td>\n","    </tr>\n","    <tr>\n","      <td>1040</td>\n","      <td>0.851500</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>0.767000</td>\n","    </tr>\n","    <tr>\n","      <td>1060</td>\n","      <td>0.820600</td>\n","    </tr>\n","    <tr>\n","      <td>1070</td>\n","      <td>0.754600</td>\n","    </tr>\n","    <tr>\n","      <td>1080</td>\n","      <td>0.739500</td>\n","    </tr>\n","    <tr>\n","      <td>1090</td>\n","      <td>0.789300</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.720400</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>0.770200</td>\n","    </tr>\n","    <tr>\n","      <td>1120</td>\n","      <td>0.775500</td>\n","    </tr>\n","    <tr>\n","      <td>1130</td>\n","      <td>0.769400</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.806300</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>0.746700</td>\n","    </tr>\n","    <tr>\n","      <td>1160</td>\n","      <td>0.767800</td>\n","    </tr>\n","    <tr>\n","      <td>1170</td>\n","      <td>0.722700</td>\n","    </tr>\n","    <tr>\n","      <td>1180</td>\n","      <td>0.766800</td>\n","    </tr>\n","    <tr>\n","      <td>1190</td>\n","      <td>0.755600</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.736100</td>\n","    </tr>\n","    <tr>\n","      <td>1210</td>\n","      <td>0.749600</td>\n","    </tr>\n","    <tr>\n","      <td>1220</td>\n","      <td>0.705800</td>\n","    </tr>\n","    <tr>\n","      <td>1230</td>\n","      <td>0.775900</td>\n","    </tr>\n","    <tr>\n","      <td>1240</td>\n","      <td>0.736600</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.730200</td>\n","    </tr>\n","    <tr>\n","      <td>1260</td>\n","      <td>0.682900</td>\n","    </tr>\n","    <tr>\n","      <td>1270</td>\n","      <td>0.701600</td>\n","    </tr>\n","    <tr>\n","      <td>1280</td>\n","      <td>0.749300</td>\n","    </tr>\n","    <tr>\n","      <td>1290</td>\n","      <td>0.681800</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.775200</td>\n","    </tr>\n","    <tr>\n","      <td>1310</td>\n","      <td>0.731800</td>\n","    </tr>\n","    <tr>\n","      <td>1320</td>\n","      <td>0.699900</td>\n","    </tr>\n","    <tr>\n","      <td>1330</td>\n","      <td>0.725300</td>\n","    </tr>\n","    <tr>\n","      <td>1340</td>\n","      <td>0.670500</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>0.665100</td>\n","    </tr>\n","    <tr>\n","      <td>1360</td>\n","      <td>0.706800</td>\n","    </tr>\n","    <tr>\n","      <td>1370</td>\n","      <td>0.605900</td>\n","    </tr>\n","    <tr>\n","      <td>1380</td>\n","      <td>0.636000</td>\n","    </tr>\n","    <tr>\n","      <td>1390</td>\n","      <td>0.659600</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.660500</td>\n","    </tr>\n","    <tr>\n","      <td>1410</td>\n","      <td>0.624500</td>\n","    </tr>\n","    <tr>\n","      <td>1420</td>\n","      <td>0.646300</td>\n","    </tr>\n","    <tr>\n","      <td>1430</td>\n","      <td>0.597500</td>\n","    </tr>\n","    <tr>\n","      <td>1440</td>\n","      <td>0.623300</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>0.636800</td>\n","    </tr>\n","    <tr>\n","      <td>1460</td>\n","      <td>0.675900</td>\n","    </tr>\n","    <tr>\n","      <td>1470</td>\n","      <td>0.648500</td>\n","    </tr>\n","    <tr>\n","      <td>1480</td>\n","      <td>0.597100</td>\n","    </tr>\n","    <tr>\n","      <td>1490</td>\n","      <td>0.630500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.622800</td>\n","    </tr>\n","    <tr>\n","      <td>1510</td>\n","      <td>0.601700</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>0.617800</td>\n","    </tr>\n","    <tr>\n","      <td>1530</td>\n","      <td>0.586700</td>\n","    </tr>\n","    <tr>\n","      <td>1540</td>\n","      <td>0.593700</td>\n","    </tr>\n","    <tr>\n","      <td>1550</td>\n","      <td>0.654000</td>\n","    </tr>\n","    <tr>\n","      <td>1560</td>\n","      <td>0.647300</td>\n","    </tr>\n","    <tr>\n","      <td>1570</td>\n","      <td>0.651300</td>\n","    </tr>\n","    <tr>\n","      <td>1580</td>\n","      <td>0.636600</td>\n","    </tr>\n","    <tr>\n","      <td>1590</td>\n","      <td>0.607800</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.636900</td>\n","    </tr>\n","    <tr>\n","      <td>1610</td>\n","      <td>0.608000</td>\n","    </tr>\n","    <tr>\n","      <td>1620</td>\n","      <td>0.571700</td>\n","    </tr>\n","    <tr>\n","      <td>1630</td>\n","      <td>0.536800</td>\n","    </tr>\n","    <tr>\n","      <td>1640</td>\n","      <td>0.530400</td>\n","    </tr>\n","    <tr>\n","      <td>1650</td>\n","      <td>0.572100</td>\n","    </tr>\n","    <tr>\n","      <td>1660</td>\n","      <td>0.512100</td>\n","    </tr>\n","    <tr>\n","      <td>1670</td>\n","      <td>0.639100</td>\n","    </tr>\n","    <tr>\n","      <td>1680</td>\n","      <td>0.565000</td>\n","    </tr>\n","    <tr>\n","      <td>1690</td>\n","      <td>0.569000</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.534900</td>\n","    </tr>\n","    <tr>\n","      <td>1710</td>\n","      <td>0.559300</td>\n","    </tr>\n","    <tr>\n","      <td>1720</td>\n","      <td>0.560700</td>\n","    </tr>\n","    <tr>\n","      <td>1730</td>\n","      <td>0.495400</td>\n","    </tr>\n","    <tr>\n","      <td>1740</td>\n","      <td>0.551800</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.582100</td>\n","    </tr>\n","    <tr>\n","      <td>1760</td>\n","      <td>0.532400</td>\n","    </tr>\n","    <tr>\n","      <td>1770</td>\n","      <td>0.557200</td>\n","    </tr>\n","    <tr>\n","      <td>1780</td>\n","      <td>0.538900</td>\n","    </tr>\n","    <tr>\n","      <td>1790</td>\n","      <td>0.524200</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.559400</td>\n","    </tr>\n","    <tr>\n","      <td>1810</td>\n","      <td>0.524300</td>\n","    </tr>\n","    <tr>\n","      <td>1820</td>\n","      <td>0.538200</td>\n","    </tr>\n","    <tr>\n","      <td>1830</td>\n","      <td>0.559700</td>\n","    </tr>\n","    <tr>\n","      <td>1840</td>\n","      <td>0.563300</td>\n","    </tr>\n","    <tr>\n","      <td>1850</td>\n","      <td>0.529300</td>\n","    </tr>\n","    <tr>\n","      <td>1860</td>\n","      <td>0.530500</td>\n","    </tr>\n","    <tr>\n","      <td>1870</td>\n","      <td>0.508800</td>\n","    </tr>\n","    <tr>\n","      <td>1880</td>\n","      <td>0.533300</td>\n","    </tr>\n","    <tr>\n","      <td>1890</td>\n","      <td>0.510200</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.551600</td>\n","    </tr>\n","    <tr>\n","      <td>1910</td>\n","      <td>0.529100</td>\n","    </tr>\n","    <tr>\n","      <td>1920</td>\n","      <td>0.499000</td>\n","    </tr>\n","    <tr>\n","      <td>1930</td>\n","      <td>0.496900</td>\n","    </tr>\n","    <tr>\n","      <td>1940</td>\n","      <td>0.508000</td>\n","    </tr>\n","    <tr>\n","      <td>1950</td>\n","      <td>0.514600</td>\n","    </tr>\n","    <tr>\n","      <td>1960</td>\n","      <td>0.553200</td>\n","    </tr>\n","    <tr>\n","      <td>1970</td>\n","      <td>0.535100</td>\n","    </tr>\n","    <tr>\n","      <td>1980</td>\n","      <td>0.529500</td>\n","    </tr>\n","    <tr>\n","      <td>1990</td>\n","      <td>0.502300</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.536600</td>\n","    </tr>\n","    <tr>\n","      <td>2010</td>\n","      <td>0.498800</td>\n","    </tr>\n","    <tr>\n","      <td>2020</td>\n","      <td>0.532600</td>\n","    </tr>\n","    <tr>\n","      <td>2030</td>\n","      <td>0.473900</td>\n","    </tr>\n","    <tr>\n","      <td>2040</td>\n","      <td>0.446100</td>\n","    </tr>\n","    <tr>\n","      <td>2050</td>\n","      <td>0.547600</td>\n","    </tr>\n","    <tr>\n","      <td>2060</td>\n","      <td>0.530000</td>\n","    </tr>\n","    <tr>\n","      <td>2070</td>\n","      <td>0.494100</td>\n","    </tr>\n","    <tr>\n","      <td>2080</td>\n","      <td>0.452900</td>\n","    </tr>\n","    <tr>\n","      <td>2090</td>\n","      <td>0.476700</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.472100</td>\n","    </tr>\n","    <tr>\n","      <td>2110</td>\n","      <td>0.492200</td>\n","    </tr>\n","    <tr>\n","      <td>2120</td>\n","      <td>0.495900</td>\n","    </tr>\n","    <tr>\n","      <td>2130</td>\n","      <td>0.462900</td>\n","    </tr>\n","    <tr>\n","      <td>2140</td>\n","      <td>0.519600</td>\n","    </tr>\n","    <tr>\n","      <td>2150</td>\n","      <td>0.474800</td>\n","    </tr>\n","    <tr>\n","      <td>2160</td>\n","      <td>0.503700</td>\n","    </tr>\n","    <tr>\n","      <td>2170</td>\n","      <td>0.497800</td>\n","    </tr>\n","    <tr>\n","      <td>2180</td>\n","      <td>0.531900</td>\n","    </tr>\n","    <tr>\n","      <td>2190</td>\n","      <td>0.489200</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.460600</td>\n","    </tr>\n","    <tr>\n","      <td>2210</td>\n","      <td>0.439200</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>0.438800</td>\n","    </tr>\n","    <tr>\n","      <td>2230</td>\n","      <td>0.476800</td>\n","    </tr>\n","    <tr>\n","      <td>2240</td>\n","      <td>0.486700</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.477900</td>\n","    </tr>\n","    <tr>\n","      <td>2260</td>\n","      <td>0.479400</td>\n","    </tr>\n","    <tr>\n","      <td>2270</td>\n","      <td>0.489800</td>\n","    </tr>\n","    <tr>\n","      <td>2280</td>\n","      <td>0.442800</td>\n","    </tr>\n","    <tr>\n","      <td>2290</td>\n","      <td>0.493100</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.479100</td>\n","    </tr>\n","    <tr>\n","      <td>2310</td>\n","      <td>0.454400</td>\n","    </tr>\n","    <tr>\n","      <td>2320</td>\n","      <td>0.425000</td>\n","    </tr>\n","    <tr>\n","      <td>2330</td>\n","      <td>0.459500</td>\n","    </tr>\n","    <tr>\n","      <td>2340</td>\n","      <td>0.467900</td>\n","    </tr>\n","    <tr>\n","      <td>2350</td>\n","      <td>0.491500</td>\n","    </tr>\n","    <tr>\n","      <td>2360</td>\n","      <td>0.419400</td>\n","    </tr>\n","    <tr>\n","      <td>2370</td>\n","      <td>0.474500</td>\n","    </tr>\n","    <tr>\n","      <td>2380</td>\n","      <td>0.441200</td>\n","    </tr>\n","    <tr>\n","      <td>2390</td>\n","      <td>0.500600</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.445600</td>\n","    </tr>\n","    <tr>\n","      <td>2410</td>\n","      <td>0.441600</td>\n","    </tr>\n","    <tr>\n","      <td>2420</td>\n","      <td>0.465100</td>\n","    </tr>\n","    <tr>\n","      <td>2430</td>\n","      <td>0.471100</td>\n","    </tr>\n","    <tr>\n","      <td>2440</td>\n","      <td>0.445000</td>\n","    </tr>\n","    <tr>\n","      <td>2450</td>\n","      <td>0.409600</td>\n","    </tr>\n","    <tr>\n","      <td>2460</td>\n","      <td>0.476500</td>\n","    </tr>\n","    <tr>\n","      <td>2470</td>\n","      <td>0.471800</td>\n","    </tr>\n","    <tr>\n","      <td>2480</td>\n","      <td>0.495400</td>\n","    </tr>\n","    <tr>\n","      <td>2490</td>\n","      <td>0.413900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.458700</td>\n","    </tr>\n","    <tr>\n","      <td>2510</td>\n","      <td>0.461900</td>\n","    </tr>\n","    <tr>\n","      <td>2520</td>\n","      <td>0.457500</td>\n","    </tr>\n","    <tr>\n","      <td>2530</td>\n","      <td>0.464300</td>\n","    </tr>\n","    <tr>\n","      <td>2540</td>\n","      <td>0.473100</td>\n","    </tr>\n","    <tr>\n","      <td>2550</td>\n","      <td>0.419200</td>\n","    </tr>\n","    <tr>\n","      <td>2560</td>\n","      <td>0.445800</td>\n","    </tr>\n","    <tr>\n","      <td>2570</td>\n","      <td>0.426700</td>\n","    </tr>\n","    <tr>\n","      <td>2580</td>\n","      <td>0.414500</td>\n","    </tr>\n","    <tr>\n","      <td>2590</td>\n","      <td>0.452500</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.404300</td>\n","    </tr>\n","    <tr>\n","      <td>2610</td>\n","      <td>0.426600</td>\n","    </tr>\n","    <tr>\n","      <td>2620</td>\n","      <td>0.433900</td>\n","    </tr>\n","    <tr>\n","      <td>2630</td>\n","      <td>0.496700</td>\n","    </tr>\n","    <tr>\n","      <td>2640</td>\n","      <td>0.436500</td>\n","    </tr>\n","    <tr>\n","      <td>2650</td>\n","      <td>0.446700</td>\n","    </tr>\n","    <tr>\n","      <td>2660</td>\n","      <td>0.464400</td>\n","    </tr>\n","    <tr>\n","      <td>2670</td>\n","      <td>0.454300</td>\n","    </tr>\n","    <tr>\n","      <td>2680</td>\n","      <td>0.423400</td>\n","    </tr>\n","    <tr>\n","      <td>2690</td>\n","      <td>0.442300</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.467600</td>\n","    </tr>\n","    <tr>\n","      <td>2710</td>\n","      <td>0.432000</td>\n","    </tr>\n","    <tr>\n","      <td>2720</td>\n","      <td>0.441300</td>\n","    </tr>\n","    <tr>\n","      <td>2730</td>\n","      <td>0.404500</td>\n","    </tr>\n","    <tr>\n","      <td>2740</td>\n","      <td>0.419100</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>0.446500</td>\n","    </tr>\n","    <tr>\n","      <td>2760</td>\n","      <td>0.410000</td>\n","    </tr>\n","    <tr>\n","      <td>2770</td>\n","      <td>0.401100</td>\n","    </tr>\n","    <tr>\n","      <td>2780</td>\n","      <td>0.428800</td>\n","    </tr>\n","    <tr>\n","      <td>2790</td>\n","      <td>0.411600</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.456900</td>\n","    </tr>\n","    <tr>\n","      <td>2810</td>\n","      <td>0.451500</td>\n","    </tr>\n","    <tr>\n","      <td>2820</td>\n","      <td>0.414700</td>\n","    </tr>\n","    <tr>\n","      <td>2830</td>\n","      <td>0.472800</td>\n","    </tr>\n","    <tr>\n","      <td>2840</td>\n","      <td>0.427700</td>\n","    </tr>\n","    <tr>\n","      <td>2850</td>\n","      <td>0.454000</td>\n","    </tr>\n","    <tr>\n","      <td>2860</td>\n","      <td>0.429000</td>\n","    </tr>\n","    <tr>\n","      <td>2870</td>\n","      <td>0.407200</td>\n","    </tr>\n","    <tr>\n","      <td>2880</td>\n","      <td>0.476900</td>\n","    </tr>\n","    <tr>\n","      <td>2890</td>\n","      <td>0.469100</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.436100</td>\n","    </tr>\n","    <tr>\n","      <td>2910</td>\n","      <td>0.454700</td>\n","    </tr>\n","    <tr>\n","      <td>2920</td>\n","      <td>0.422400</td>\n","    </tr>\n","    <tr>\n","      <td>2930</td>\n","      <td>0.386300</td>\n","    </tr>\n","    <tr>\n","      <td>2940</td>\n","      <td>0.456100</td>\n","    </tr>\n","    <tr>\n","      <td>2950</td>\n","      <td>0.455600</td>\n","    </tr>\n","    <tr>\n","      <td>2960</td>\n","      <td>0.403700</td>\n","    </tr>\n","    <tr>\n","      <td>2970</td>\n","      <td>0.439100</td>\n","    </tr>\n","    <tr>\n","      <td>2980</td>\n","      <td>0.404200</td>\n","    </tr>\n","    <tr>\n","      <td>2990</td>\n","      <td>0.395500</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.441000</td>\n","    </tr>\n","    <tr>\n","      <td>3010</td>\n","      <td>0.401600</td>\n","    </tr>\n","    <tr>\n","      <td>3020</td>\n","      <td>0.434400</td>\n","    </tr>\n","    <tr>\n","      <td>3030</td>\n","      <td>0.370100</td>\n","    </tr>\n","    <tr>\n","      <td>3040</td>\n","      <td>0.445800</td>\n","    </tr>\n","    <tr>\n","      <td>3050</td>\n","      <td>0.441700</td>\n","    </tr>\n","    <tr>\n","      <td>3060</td>\n","      <td>0.465800</td>\n","    </tr>\n","    <tr>\n","      <td>3070</td>\n","      <td>0.424900</td>\n","    </tr>\n","    <tr>\n","      <td>3080</td>\n","      <td>0.443900</td>\n","    </tr>\n","    <tr>\n","      <td>3090</td>\n","      <td>0.440000</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.420800</td>\n","    </tr>\n","    <tr>\n","      <td>3110</td>\n","      <td>0.449200</td>\n","    </tr>\n","    <tr>\n","      <td>3120</td>\n","      <td>0.437100</td>\n","    </tr>\n","    <tr>\n","      <td>3130</td>\n","      <td>0.415700</td>\n","    </tr>\n","    <tr>\n","      <td>3140</td>\n","      <td>0.410800</td>\n","    </tr>\n","    <tr>\n","      <td>3150</td>\n","      <td>0.418500</td>\n","    </tr>\n","    <tr>\n","      <td>3160</td>\n","      <td>0.381300</td>\n","    </tr>\n","    <tr>\n","      <td>3170</td>\n","      <td>0.404500</td>\n","    </tr>\n","    <tr>\n","      <td>3180</td>\n","      <td>0.429000</td>\n","    </tr>\n","    <tr>\n","      <td>3190</td>\n","      <td>0.405900</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.435800</td>\n","    </tr>\n","    <tr>\n","      <td>3210</td>\n","      <td>0.393300</td>\n","    </tr>\n","    <tr>\n","      <td>3220</td>\n","      <td>0.440000</td>\n","    </tr>\n","    <tr>\n","      <td>3230</td>\n","      <td>0.398900</td>\n","    </tr>\n","    <tr>\n","      <td>3240</td>\n","      <td>0.446500</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>0.424400</td>\n","    </tr>\n","    <tr>\n","      <td>3260</td>\n","      <td>0.381800</td>\n","    </tr>\n","    <tr>\n","      <td>3270</td>\n","      <td>0.387900</td>\n","    </tr>\n","    <tr>\n","      <td>3280</td>\n","      <td>0.417900</td>\n","    </tr>\n","    <tr>\n","      <td>3290</td>\n","      <td>0.436400</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.407100</td>\n","    </tr>\n","    <tr>\n","      <td>3310</td>\n","      <td>0.406200</td>\n","    </tr>\n","    <tr>\n","      <td>3320</td>\n","      <td>0.409500</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>0.389600</td>\n","    </tr>\n","    <tr>\n","      <td>3340</td>\n","      <td>0.415000</td>\n","    </tr>\n","    <tr>\n","      <td>3350</td>\n","      <td>0.407500</td>\n","    </tr>\n","    <tr>\n","      <td>3360</td>\n","      <td>0.439900</td>\n","    </tr>\n","    <tr>\n","      <td>3370</td>\n","      <td>0.417600</td>\n","    </tr>\n","    <tr>\n","      <td>3380</td>\n","      <td>0.414100</td>\n","    </tr>\n","    <tr>\n","      <td>3390</td>\n","      <td>0.383200</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.385500</td>\n","    </tr>\n","    <tr>\n","      <td>3410</td>\n","      <td>0.399500</td>\n","    </tr>\n","    <tr>\n","      <td>3420</td>\n","      <td>0.390300</td>\n","    </tr>\n","    <tr>\n","      <td>3430</td>\n","      <td>0.391300</td>\n","    </tr>\n","    <tr>\n","      <td>3440</td>\n","      <td>0.419100</td>\n","    </tr>\n","    <tr>\n","      <td>3450</td>\n","      <td>0.444900</td>\n","    </tr>\n","    <tr>\n","      <td>3460</td>\n","      <td>0.390700</td>\n","    </tr>\n","    <tr>\n","      <td>3470</td>\n","      <td>0.412100</td>\n","    </tr>\n","    <tr>\n","      <td>3480</td>\n","      <td>0.411300</td>\n","    </tr>\n","    <tr>\n","      <td>3490</td>\n","      <td>0.436600</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.397300</td>\n","    </tr>\n","    <tr>\n","      <td>3510</td>\n","      <td>0.422100</td>\n","    </tr>\n","    <tr>\n","      <td>3520</td>\n","      <td>0.393200</td>\n","    </tr>\n","    <tr>\n","      <td>3530</td>\n","      <td>0.396600</td>\n","    </tr>\n","    <tr>\n","      <td>3540</td>\n","      <td>0.381800</td>\n","    </tr>\n","    <tr>\n","      <td>3550</td>\n","      <td>0.405100</td>\n","    </tr>\n","    <tr>\n","      <td>3560</td>\n","      <td>0.366100</td>\n","    </tr>\n","    <tr>\n","      <td>3570</td>\n","      <td>0.380000</td>\n","    </tr>\n","    <tr>\n","      <td>3580</td>\n","      <td>0.395300</td>\n","    </tr>\n","    <tr>\n","      <td>3590</td>\n","      <td>0.383900</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.427300</td>\n","    </tr>\n","    <tr>\n","      <td>3610</td>\n","      <td>0.328700</td>\n","    </tr>\n","    <tr>\n","      <td>3620</td>\n","      <td>0.410200</td>\n","    </tr>\n","    <tr>\n","      <td>3630</td>\n","      <td>0.399600</td>\n","    </tr>\n","    <tr>\n","      <td>3640</td>\n","      <td>0.391800</td>\n","    </tr>\n","    <tr>\n","      <td>3650</td>\n","      <td>0.402800</td>\n","    </tr>\n","    <tr>\n","      <td>3660</td>\n","      <td>0.424900</td>\n","    </tr>\n","    <tr>\n","      <td>3670</td>\n","      <td>0.432600</td>\n","    </tr>\n","    <tr>\n","      <td>3680</td>\n","      <td>0.373400</td>\n","    </tr>\n","    <tr>\n","      <td>3690</td>\n","      <td>0.384600</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.390000</td>\n","    </tr>\n","    <tr>\n","      <td>3710</td>\n","      <td>0.369000</td>\n","    </tr>\n","    <tr>\n","      <td>3720</td>\n","      <td>0.409700</td>\n","    </tr>\n","    <tr>\n","      <td>3730</td>\n","      <td>0.439800</td>\n","    </tr>\n","    <tr>\n","      <td>3740</td>\n","      <td>0.398600</td>\n","    </tr>\n","    <tr>\n","      <td>3750</td>\n","      <td>0.421600</td>\n","    </tr>\n","    <tr>\n","      <td>3760</td>\n","      <td>0.398000</td>\n","    </tr>\n","    <tr>\n","      <td>3770</td>\n","      <td>0.381000</td>\n","    </tr>\n","    <tr>\n","      <td>3780</td>\n","      <td>0.351800</td>\n","    </tr>\n","    <tr>\n","      <td>3790</td>\n","      <td>0.406300</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.397700</td>\n","    </tr>\n","    <tr>\n","      <td>3810</td>\n","      <td>0.383700</td>\n","    </tr>\n","    <tr>\n","      <td>3820</td>\n","      <td>0.417100</td>\n","    </tr>\n","    <tr>\n","      <td>3830</td>\n","      <td>0.392100</td>\n","    </tr>\n","    <tr>\n","      <td>3840</td>\n","      <td>0.409100</td>\n","    </tr>\n","    <tr>\n","      <td>3850</td>\n","      <td>0.386400</td>\n","    </tr>\n","    <tr>\n","      <td>3860</td>\n","      <td>0.400300</td>\n","    </tr>\n","    <tr>\n","      <td>3870</td>\n","      <td>0.396500</td>\n","    </tr>\n","    <tr>\n","      <td>3880</td>\n","      <td>0.395000</td>\n","    </tr>\n","    <tr>\n","      <td>3890</td>\n","      <td>0.434400</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>0.405100</td>\n","    </tr>\n","    <tr>\n","      <td>3910</td>\n","      <td>0.440000</td>\n","    </tr>\n","    <tr>\n","      <td>3920</td>\n","      <td>0.389300</td>\n","    </tr>\n","    <tr>\n","      <td>3930</td>\n","      <td>0.396600</td>\n","    </tr>\n","    <tr>\n","      <td>3940</td>\n","      <td>0.397800</td>\n","    </tr>\n","    <tr>\n","      <td>3950</td>\n","      <td>0.408700</td>\n","    </tr>\n","    <tr>\n","      <td>3960</td>\n","      <td>0.384800</td>\n","    </tr>\n","    <tr>\n","      <td>3970</td>\n","      <td>0.422000</td>\n","    </tr>\n","    <tr>\n","      <td>3980</td>\n","      <td>0.392300</td>\n","    </tr>\n","    <tr>\n","      <td>3990</td>\n","      <td>0.455300</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.381800</td>\n","    </tr>\n","    <tr>\n","      <td>4010</td>\n","      <td>0.416000</td>\n","    </tr>\n","    <tr>\n","      <td>4020</td>\n","      <td>0.371400</td>\n","    </tr>\n","    <tr>\n","      <td>4030</td>\n","      <td>0.409400</td>\n","    </tr>\n","    <tr>\n","      <td>4040</td>\n","      <td>0.384100</td>\n","    </tr>\n","    <tr>\n","      <td>4050</td>\n","      <td>0.399900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["# ✅ Testes do modelo\n","def testar_modelo(prompt_input):\n","    input_text = \"Traduza para o português: \" + prompt_input\n","    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n","    output = model.generate(input_ids, max_new_tokens=30, do_sample=True)\n","    print(f\"> Input: {prompt_input}\")\n","    print(\"> Tradução:\", tokenizer.decode(output[0], skip_special_tokens=True))\n","\n","# Exemplos de teste\n","testar_modelo(\"Djety-mbowé\")\n","testar_modelo(\"Kamby\")\n","testar_modelo(\"Eí\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3kPz6Le9mVH","executionInfo":{"status":"ok","timestamp":1751164533818,"user_tz":180,"elapsed":1063,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"23546ad9-3df3-4522-f539-75c54949a8cf"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["> Input: Djety-mbowé\n","> Tradução: Traduza para o português: Djety-mbowé banha! Banha ky-re'ẽ! Vai com Deus Petapeó larva do che! Vai com Deus\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["> Input: Kamby\n","> Tradução: Traduza para o português: Kamby leite Kamby leite Como viviam Nhaningarekhe'ẽ. Como viviam Nhe'\n","> Input: Eí\n","> Tradução: Traduza para o português: Eí mel de abelhas até. Vicho animal aldeia Nossa flor é muito bela. Upegwi adj\n"]}]},{"cell_type":"code","source":["#!pip install evaluate sacrebleu\n","import evaluate\n","import numpy as np\n","\n","# Carregar métricas\n","bleu = evaluate.load(\"sacrebleu\")\n","\n","# Gerar previsões para o conjunto de teste\n","samples = df.sample(50, random_state=42)  # ou usar parte de `augmented_data`\n","\n","referencias = []\n","predicoes = []\n","\n","for _, row in samples.iterrows():\n","    entrada = row[\"input\"]\n","    referencia = row[\"output\"]\n","\n","    input_text = \"Traduza para o português: \" + entrada\n","    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n","    output = model.generate(input_ids, max_new_tokens=30, do_sample=False)\n","    saida = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","    # Pós-processamento para pegar só a tradução\n","    traducao = saida.replace(input_text, \"\").strip()\n","\n","    referencias.append([referencia])\n","    predicoes.append(traducao)\n","\n","# Calcular BLEU\n","bleu_result = bleu.compute(predictions=predicoes, references=referencias)\n","print(f\"📘 BLEU score: {bleu_result['score']:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iFC9HWxUzXVT","executionInfo":{"status":"ok","timestamp":1751164555559,"user_tz":180,"elapsed":21733,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"652d6979-d837-4cc1-a2c0-c09efc554c5e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["📘 BLEU score: 7.86\n"]}]},{"cell_type":"code","source":["from torch.nn import CrossEntropyLoss\n","\n","def calcular_perplexity(model, dataset_tokenized, n=100):\n","    model.eval()\n","    losses = []\n","    # converte para dicionário com listas para cada chave\n","    batches = dataset_tokenized.select(range(n)).to_dict()\n","    for i in range(n):\n","        input_ids = batches['input_ids'][i]\n","        labels = batches['labels'][i]\n","        inputs = torch.tensor([input_ids]).to(model.device)\n","        labels = torch.tensor([labels]).to(model.device)\n","        with torch.no_grad():\n","            outputs = model(inputs, labels=labels)\n","        losses.append(outputs.loss.item())\n","    mean_loss = np.mean(losses)\n","    perplexity = np.exp(mean_loss)\n","    print(f\"🔢 Perplexity: {perplexity:.2f}\")\n","    return perplexity\n","\n","# Chamada:\n","calcular_perplexity(model, dataset_tokenized)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"siSObFRhzN8g","executionInfo":{"status":"ok","timestamp":1751164557962,"user_tz":180,"elapsed":2400,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"6c0c8a79-17f9-4a8e-a927-b4b9a32c0c2b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["🔢 Perplexity: 77259878116.51\n"]},{"output_type":"execute_result","data":{"text/plain":["np.float64(77259878116.50635)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","\n","# ✅ Caminho do modelo salvo no seu Google Drive\n","caminho_modelo = \"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/guarani_gpt2\"\n","\n","# ✅ Carregar modelo e tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(caminho_modelo)\n","model = GPT2LMHeadModel.from_pretrained(caminho_modelo)\n","model.eval()\n","\n","# ✅ Enviar modelo para GPU se disponível\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# ✅ Função de tradução\n","attention_mask = torch.ones_like(input_ids)\n","# Configurar pad_token\n","tokenizer.pad_token = tokenizer.eos_token\n","model.config.pad_token_id = tokenizer.eos_token_id\n","\n","def traduzir_guarani(texto_guarani):\n","    prompt = \"Traduza para o português: \" + texto_guarani\n","    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n","    output = model.generate(input_ids,\n","                            attention_mask=attention_mask,\n","                            max_length=50,\n","                            num_beams=4,\n","                            max_new_tokens=30,\n","                            do_sample=True,\n","                            early_stopping=True,\n","                            no_repeat_ngram_size=2,\n","                            temperature=0.5)\n","    traducao = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return traducao\n","\n","# ✅ Loop interativo\n","print(\"🔡 Digite uma palavra/frase em Guarani (ou 'sair' para encerrar):\")\n","while True:\n","    entrada = input(\"📝 Guarani: \")\n","    if entrada.lower() in [\"sair\", \"exit\", \"quit\"]:\n","        print(\"👋 Encerrando.\")\n","        break\n","    resposta = traduzir_guarani(entrada)\n","    print(\"📘 Tradução:\", resposta)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"id":"W0PZUlmBaPnD","executionInfo":{"status":"error","timestamp":1751164797606,"user_tz":180,"elapsed":168980,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"0b667f41-976c-4c3d-c3ae-fee2b55a9e3b"},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":["🔡 Digite uma palavra/frase em Guarani (ou 'sair' para encerrar):\n","📝 Guarani: menino\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Both `max_new_tokens` (=30) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["📘 Tradução: Traduza para o português: menino nandadadurururanuraniruraniruiruirirunir uniriru uniru uiru Uiru Uniru Unioniru Universal\n","📝 Guarani: Bom dia\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Both `max_new_tokens` (=30) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["📘 Tradução: Traduza para o português: Bom dia j jjjajajjawajawawadawadaawdawbyawandawwereandandwereadandareandad\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-18-468140770.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🔡 Digite uma palavra/frase em Guarani (ou 'sair' para encerrar):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mentrada\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"📝 Guarani: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mentrada\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"sair\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"👋 Encerrando.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":["def traduzir_corrigido(entrada):\n","    input_text = f\"Traduza para o português: {entrada}\"\n","    inputs = tokenizer(input_text, return_tensors=\"pt\")\n","\n","    output = model.generate(\n","        inputs.input_ids,\n","        attention_mask=inputs.attention_mask,\n","        max_new_tokens=15,\n","        temperature=1.0,\n","        repetition_penalty=1.5,\n","        no_repeat_ngram_size=2,\n","        do_sample=True,\n","        top_p=0.8\n","    )\n","\n","    result = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return result.replace(input_text, \"\").strip()\n","\n","# Teste\n","print(traduzir_corrigido(\"Kamby\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"jNtDgT3hjf8y","executionInfo":{"status":"error","timestamp":1751164800808,"user_tz":180,"elapsed":150,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"90d4fb7a-10b0-4f58-a2f0-506fdea3bafc"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-19-2390786966.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraduzir_corrigido\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Kamby\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-19-2390786966.py\u001b[0m in \u001b[0;36mtraduzir_corrigido\u001b[0;34m(entrada)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     output = model.generate(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2598\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3557\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3558\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1210\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1211\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcache_position\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"]}]}]}