{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyMgIn33qvPPBHSMfmOIl7SK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0899a043970f4cb288a26c3f86178424":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b581717d355c4a618e78724b7acc218d","IPY_MODEL_e27c28b2ea834b65a06692f9843df1ab","IPY_MODEL_ce5f4fcc3a274a99ae0107580d35b94f"],"layout":"IPY_MODEL_9e4819544c134cb98bb234d0f1848a80"}},"b581717d355c4a618e78724b7acc218d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_352cb2e334fb42d1b135f10fd8ddac01","placeholder":"​","style":"IPY_MODEL_b4fcceaea5f34c92a379694cc9d863f5","value":"Loading checkpoint shards: 100%"}},"e27c28b2ea834b65a06692f9843df1ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4b11aac23c8402b9892bd515d67b524","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e44d8cb1cf9448b9d3a0e8bc0ac4fda","value":2}},"ce5f4fcc3a274a99ae0107580d35b94f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cab9080815da469eae79dc01df4437af","placeholder":"​","style":"IPY_MODEL_35affd4c71744d79b2cce97558437a49","value":" 2/2 [00:01&lt;00:00,  1.69s/it]"}},"9e4819544c134cb98bb234d0f1848a80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"352cb2e334fb42d1b135f10fd8ddac01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4fcceaea5f34c92a379694cc9d863f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4b11aac23c8402b9892bd515d67b524":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e44d8cb1cf9448b9d3a0e8bc0ac4fda":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cab9080815da469eae79dc01df4437af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35affd4c71744d79b2cce97558437a49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"920736d9e0f748f2aecac60b3a0fbe4e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3f5b003f27aa41bb9328f917711241da","IPY_MODEL_1d67739e532747178dd71feb07eaa57e","IPY_MODEL_8f35a15e89254ad39f88d71cb85cda23"],"layout":"IPY_MODEL_d4c935f775524d4ea50fe668f42ca9e8"}},"3f5b003f27aa41bb9328f917711241da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4373dd5a5a34d25aaedf1a657e3e435","placeholder":"​","style":"IPY_MODEL_c2dcb9d702864c8192b166aac611fb97","value":"Map: 100%"}},"1d67739e532747178dd71feb07eaa57e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cf22c2b1cad41c4a069804a7b0d5aeb","max":224,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b96c9d0c82bf4790b852c2de95d5d73a","value":224}},"8f35a15e89254ad39f88d71cb85cda23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65812b73bcf54e9d8e644cc5208dee34","placeholder":"​","style":"IPY_MODEL_94420943149e46f2b98cef6ff36501f9","value":" 224/224 [00:00&lt;00:00, 2091.75 examples/s]"}},"d4c935f775524d4ea50fe668f42ca9e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4373dd5a5a34d25aaedf1a657e3e435":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2dcb9d702864c8192b166aac611fb97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cf22c2b1cad41c4a069804a7b0d5aeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b96c9d0c82bf4790b852c2de95d5d73a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65812b73bcf54e9d8e644cc5208dee34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94420943149e46f2b98cef6ff36501f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7f6fbbcc5bd49c9a33bba44882aa6ae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c959181174843708a4addec71c5e394","IPY_MODEL_63aae387b09b4511817b99f1d38699cf","IPY_MODEL_2be56c8197d74fb09d98ea41b9fccb5c"],"layout":"IPY_MODEL_636e24618c3c4bed91218fe6f89f12a0"}},"4c959181174843708a4addec71c5e394":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f08d4a8bdb74964aa155f8421c53102","placeholder":"​","style":"IPY_MODEL_2b5c1ca4f9454f52a5310c505137af1a","value":"Map: 100%"}},"63aae387b09b4511817b99f1d38699cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72bca3ee35f84a77896b59907f105e4e","max":57,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ba6dea8e42c5447a8d5e99a974928025","value":57}},"2be56c8197d74fb09d98ea41b9fccb5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_222b611d48b74c1c92177297d3381faf","placeholder":"​","style":"IPY_MODEL_473395f1ffe84e97bcc0353da8deae49","value":" 57/57 [00:00&lt;00:00, 1565.42 examples/s]"}},"636e24618c3c4bed91218fe6f89f12a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f08d4a8bdb74964aa155f8421c53102":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b5c1ca4f9454f52a5310c505137af1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72bca3ee35f84a77896b59907f105e4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba6dea8e42c5447a8d5e99a974928025":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"222b611d48b74c1c92177297d3381faf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"473395f1ffe84e97bcc0353da8deae49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0899a043970f4cb288a26c3f86178424","b581717d355c4a618e78724b7acc218d","e27c28b2ea834b65a06692f9843df1ab","ce5f4fcc3a274a99ae0107580d35b94f","9e4819544c134cb98bb234d0f1848a80","352cb2e334fb42d1b135f10fd8ddac01","b4fcceaea5f34c92a379694cc9d863f5","f4b11aac23c8402b9892bd515d67b524","3e44d8cb1cf9448b9d3a0e8bc0ac4fda","cab9080815da469eae79dc01df4437af","35affd4c71744d79b2cce97558437a49","920736d9e0f748f2aecac60b3a0fbe4e","3f5b003f27aa41bb9328f917711241da","1d67739e532747178dd71feb07eaa57e","8f35a15e89254ad39f88d71cb85cda23","d4c935f775524d4ea50fe668f42ca9e8","d4373dd5a5a34d25aaedf1a657e3e435","c2dcb9d702864c8192b166aac611fb97","3cf22c2b1cad41c4a069804a7b0d5aeb","b96c9d0c82bf4790b852c2de95d5d73a","65812b73bcf54e9d8e644cc5208dee34","94420943149e46f2b98cef6ff36501f9","e7f6fbbcc5bd49c9a33bba44882aa6ae","4c959181174843708a4addec71c5e394","63aae387b09b4511817b99f1d38699cf","2be56c8197d74fb09d98ea41b9fccb5c","636e24618c3c4bed91218fe6f89f12a0","3f08d4a8bdb74964aa155f8421c53102","2b5c1ca4f9454f52a5310c505137af1a","72bca3ee35f84a77896b59907f105e4e","ba6dea8e42c5447a8d5e99a974928025","222b611d48b74c1c92177297d3381faf","473395f1ffe84e97bcc0353da8deae49"]},"id":"S8hPQ6iN_KtC","executionInfo":{"status":"error","timestamp":1751138921574,"user_tz":180,"elapsed":9156,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"1c7981d3-66d5-4d28-cee3-77153a367bd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","🔄 Carregando modelo: Emilio407/guarani-jopara-gemma-2-2b-it-v1\n","✅ Tokenizer carregado\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0899a043970f4cb288a26c3f86178424"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Modelo carregado com otimizações\n","🖥️ GPU - Alocado: 14.6GB | Reservado: 15.7GB\n","\n","🔄 Carregando e preparando dados...\n","📊 Treino: 224 | Validação: 57\n","🔄 Aplicando tokenização...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/224 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"920736d9e0f748f2aecac60b3a0fbe4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/57 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f6fbbcc5bd49c9a33bba44882aa6ae"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Tokenização concluída\n","\n","🔄 Iniciando treinamento com configurações otimizadas...\n","🖥️ GPU - Alocado: 14.6GB | Reservado: 15.7GB\n","🚀 Iniciando treinamento...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2-5183507.py:210: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"stream","name":"stdout","text":["❌ Erro durante treinamento: CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 22.16 GiB of which 1.03 GiB is free. Process 21898 has 21.13 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 304.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","💡 Tentando com configurações ainda mais conservadoras...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2-5183507.py:241: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 22.16 GiB of which 1.03 GiB is free. Process 21898 has 21.13 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 304.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2-5183507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚀 Iniciando treinamento...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Treinamento concluído!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2548\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2549\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2550\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 22.16 GiB of which 1.03 GiB is free. Process 21898 has 21.13 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 304.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2-5183507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3789\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2547\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2549\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2550\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2551\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 22.16 GiB of which 1.03 GiB is free. Process 21898 has 21.13 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 304.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n","import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","from torch.nn import CrossEntropyLoss\n","import gc\n","\n","# === CONFIGURAÇÃO OTIMIZADA DE MEMÓRIA ===\n","def cleanup_memory():\n","    \"\"\"Limpa cache da GPU e coleta garbage\"\"\"\n","    gc.collect()\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","        torch.cuda.synchronize()\n","\n","def print_gpu_utilization():\n","    \"\"\"Mostra utilização da GPU\"\"\"\n","    if torch.cuda.is_available():\n","        allocated = torch.cuda.memory_allocated() / 1024**3\n","        reserved = torch.cuda.memory_reserved() / 1024**3\n","        print(f\"🖥️ GPU - Alocado: {allocated:.1f}GB | Reservado: {reserved:.1f}GB\")\n","\n","# Configurações globais para economia de memória\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","# Limpeza inicial\n","cleanup_memory()\n","\n","drive.mount('/content/drive')\n","\n","# === CONFIGURAÇÃO DO MODELO ===\n","model_id = \"Emilio407/guarani-jopara-gemma-2-2b-it-v1\"\n","print(f\"🔄 Carregando modelo: {model_id}\")\n","\n","# Carrega tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","print(\"✅ Tokenizer carregado\")\n","\n","# Carrega modelo com otimizações\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float16,  # Usa half precision desde o carregamento\n","    device_map=\"auto\",\n","    low_cpu_mem_usage=True,\n",")\n","print(\"✅ Modelo carregado com otimizações\")\n","print_gpu_utilization()\n","\n","# === PREPARAÇÃO DOS DADOS OTIMIZADA ===\n","print(\"\\n🔄 Carregando e preparando dados...\")\n","df = pd.read_json(\"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/dados_treinamento_guarani.json\")\n","\n","# Divide os dados (usando menos dados se necessário para economizar memória)\n","if len(df) > 1000:  # Se tiver muitos dados, usa uma amostra\n","    df = df.sample(n=1000, random_state=42)\n","    print(f\"⚠️ Usando amostra de 1000 exemplos para economizar memória\")\n","\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n","print(f\"📊 Treino: {len(train_df)} | Validação: {len(val_df)}\")\n","\n","# Converte para Dataset\n","train_dataset = Dataset.from_pandas(train_df)\n","val_dataset = Dataset.from_pandas(val_df)\n","\n","# Função de tokenização otimizada\n","def tokenize(example):\n","    prompt = example[\"instruction\"] + \"\\n\" + example[\"input\"] + \"\\n\"\n","    target = example[\"output\"]\n","    full_text = prompt + target\n","\n","    # Tokeniza com max_length menor para economizar memória\n","    tokenized = tokenizer(\n","        full_text,\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=256,  # Reduzido para economizar memória\n","        return_tensors=None  # Não retorna tensors para economizar memória\n","    )\n","\n","    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n","    return tokenized\n","\n","print(\"🔄 Aplicando tokenização...\")\n","tokenized_train = train_dataset.map(\n","    tokenize,\n","    remove_columns=train_dataset.column_names,\n","    batched=False,  # Processa um por vez para economizar memória\n","    load_from_cache_file=False\n",")\n","tokenized_val = val_dataset.map(\n","    tokenize,\n","    remove_columns=val_dataset.column_names,\n","    batched=False,\n","    load_from_cache_file=False\n",")\n","\n","cleanup_memory()\n","print(\"✅ Tokenização concluída\")\n","\n","# Data collator otimizado\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n","    return_tensors=\"pt\"\n",")\n","\n","# === FUNÇÃO DE MÉTRICAS OTIMIZADA ===\n","def compute_metrics(eval_pred):\n","    \"\"\"Função de métricas otimizada para economizar memória\"\"\"\n","    predictions, labels = eval_pred\n","\n","    # Move para CPU se necessário\n","    if isinstance(predictions, torch.Tensor):\n","        predictions = predictions.detach().cpu()\n","    if isinstance(labels, torch.Tensor):\n","        labels = labels.detach().cpu()\n","\n","    # Calcula perplexity de forma mais eficiente\n","    try:\n","        shift_logits = predictions[..., :-1, :].contiguous()\n","        shift_labels = labels[..., 1:].contiguous()\n","\n","        shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n","        shift_labels = shift_labels.view(-1)\n","\n","        # Remove padding\n","        mask = shift_labels != -100\n","        shift_logits = shift_logits[mask]\n","        shift_labels = shift_labels[mask]\n","\n","        # Calcula loss\n","        loss_fct = CrossEntropyLoss()\n","        loss = loss_fct(shift_logits.float(), shift_labels)\n","        perplexity = torch.exp(loss)\n","\n","        return {\n","            \"eval_loss\": loss.item(),\n","            \"perplexity\": perplexity.item()\n","        }\n","    except Exception as e:\n","        print(f\"⚠️ Erro no cálculo de métricas: {e}\")\n","        return {\"eval_loss\": 0.0, \"perplexity\": 1000.0}\n","\n","# === ARGUMENTOS DE TREINAMENTO ULTRA-OTIMIZADOS ===\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/modelo_finetunado\",\n","\n","    # Configurações de batch otimizadas\n","    per_device_train_batch_size=1,      # Muito pequeno para economizar memória\n","    per_device_eval_batch_size=1,       # Muito pequeno para avaliação\n","    gradient_accumulation_steps=8,       # Simula batch_size=8\n","\n","    # Configurações de treinamento\n","    num_train_epochs=2,                  # Reduzido para teste inicial\n","    learning_rate=2e-5,                  # Learning rate conservador\n","    warmup_ratio=0.1,                    # Warmup para estabilidade\n","\n","    # Configurações de avaliação\n","    eval_strategy=\"steps\",\n","    eval_steps=100,                      # Avalia menos frequentemente\n","    eval_accumulation_steps=1,           # Não acumula durante avaliação\n","\n","    # Configurações de salvamento\n","    save_strategy=\"steps\",\n","    save_steps=200,                      # Salva menos frequentemente\n","    save_total_limit=1,                  # Mantém apenas 1 checkpoint\n","\n","    # Otimizações de memória CRÍTICAS\n","    fp16=False,                          # Half precision - ESSENCIAL!\n","    dataloader_pin_memory=False,        # Economiza memória\n","    gradient_checkpointing=True,        # Troca computação por memória\n","    dataloader_num_workers=0,           # Sem workers paralelos\n","    remove_unused_columns=True,         # Remove colunas desnecessárias\n","\n","    # Configurações de otimizador\n","    optim=\"adamw_torch\",                # Otimizador eficiente\n","    weight_decay=0.01,\n","    adam_epsilon=1e-8,\n","\n","    # Logging e relatórios\n","    logging_dir=\"logs\",\n","    logging_steps=50,\n","    logging_first_step=True,\n","\n","    # Configurações de modelo\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    greater_is_better=False,\n","\n","    # Desabilita relatórios externos\n","    report_to=[],\n","\n","    # Configurações adicionais para estabilidade\n","    max_grad_norm=1.0,                  # Gradient clipping\n","    prediction_loss_only=False,\n",")\n","\n","print(\"\\n🔄 Iniciando treinamento com configurações otimizadas...\")\n","print_gpu_utilization()\n","\n","# === TRAINER OTIMIZADO ===\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_val,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# === TREINAMENTO COM MONITORAMENTO ===\n","try:\n","    print(\"🚀 Iniciando treinamento...\")\n","    trainer.train()\n","    print(\"✅ Treinamento concluído!\")\n","\n","    # Salva o modelo\n","    output_dir = \"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/modelo_finetunado\"\n","    trainer.save_model(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","    print(f\"✅ Modelo salvo em: {output_dir}\")\n","\n","except Exception as e:\n","    print(f\"❌ Erro durante treinamento: {e}\")\n","    print(\"💡 Tentando com configurações ainda mais conservadoras...\")\n","\n","    # Configurações de emergência\n","    training_args.per_device_train_batch_size = 1\n","    training_args.gradient_accumulation_steps = 4\n","    training_args.num_train_epochs = 1\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train.select(range(min(100, len(tokenized_train)))),  # Usa só 100 exemplos\n","        eval_dataset=tokenized_val.select(range(min(20, len(tokenized_val)))),\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","    )\n","\n","    trainer.train()\n","    trainer.save_model(output_dir)\n","\n","# === LIMPEZA ANTES DA AVALIAÇÃO ===\n","del trainer\n","cleanup_memory()\n","print(\"\\n🧹 Memória limpa após treinamento\")\n","print_gpu_utilization()\n","\n","# === AVALIAÇÃO OTIMIZADA ===\n","print(\"\\n=== INICIANDO AVALIAÇÃO ===\")\n","\n","# Carrega modelo treinado com otimizações\n","model_path = \"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/modelo_finetunado\"\n","tokenizer_trained = AutoTokenizer.from_pretrained(model_path)\n","model_trained = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    low_cpu_mem_usage=True\n",")\n","model_trained.eval()\n","\n","print(\"✅ Modelo treinado carregado para avaliação\")\n","\n","# Função de avaliação otimizada\n","def avaliar_modelo_otimizado(perguntas_teste):\n","    \"\"\"Avalia o modelo de forma otimizada\"\"\"\n","    resultados = []\n","\n","    for i, pergunta in enumerate(perguntas_teste):\n","        try:\n","            inputs = tokenizer_trained(pergunta, return_tensors=\"pt\")\n","\n","            with torch.no_grad():\n","                outputs = model_trained.generate(\n","                    inputs['input_ids'],\n","                    max_length=80,  # Reduzido para economizar\n","                    do_sample=True,\n","                    temperature=0.7,\n","                    num_return_sequences=1,\n","                    pad_token_id=tokenizer_trained.eos_token_id,\n","                    early_stopping=True\n","                )\n","\n","            resposta = tokenizer_trained.decode(outputs[0], skip_special_tokens=True)\n","            resposta_limpa = resposta[len(pergunta):].strip()\n","\n","            resultados.append({\n","                'pergunta': pergunta,\n","                'resposta': resposta_limpa\n","            })\n","\n","            print(f\"Pergunta {i+1}: {pergunta}\")\n","            print(f\"Resposta: {resposta_limpa}\")\n","            print(\"-\" * 50)\n","\n","            # Limpa memória a cada iteração\n","            del inputs, outputs\n","            cleanup_memory()\n","\n","        except Exception as e:\n","            print(f\"⚠️ Erro na pergunta '{pergunta}': {e}\")\n","\n","    return resultados\n","\n","# Perguntas de teste\n","perguntas_teste = [\n","    \"Como se diz 'gavião' em Guarani?\",\n","    \"Como se diz 'água' em Guarani?\",\n","    \"Como se diz 'casa' em Guarani?\",\n","    \"Como se diz 'sol' em Guarani?\",\n","    \"Como se diz 'lua' em Guarani?\"\n","]\n","\n","print(\"🔄 Testando modelo treinado...\")\n","resultados = avaliar_modelo_otimizado(perguntas_teste)\n","\n","# === CÁLCULO DE PERPLEXITY OTIMIZADO ===\n","def calcular_perplexity_otimizado(model, tokenizer, dataset, max_samples=30):\n","    \"\"\"Calcula perplexity de forma otimizada\"\"\"\n","    model.eval()\n","    total_loss = 0\n","    total_tokens = 0\n","    processed = 0\n","\n","    # Usa amostra muito pequena para economizar memória\n","    indices = np.random.choice(len(dataset), min(max_samples, len(dataset)), replace=False)\n","    sample_dataset = dataset.select(indices)\n","\n","    print(f\"🔄 Calculando perplexity em {len(sample_dataset)} exemplos...\")\n","\n","    with torch.no_grad():\n","        for example in sample_dataset:\n","            try:\n","                input_ids = torch.tensor([example['input_ids']], dtype=torch.long)\n","                labels = torch.tensor([example['labels']], dtype=torch.long)\n","\n","                outputs = model(input_ids, labels=labels)\n","                loss = outputs.loss\n","\n","                # Conta tokens válidos\n","                mask = labels != -100\n","                num_tokens = mask.sum().item()\n","\n","                if num_tokens > 0:\n","                    total_loss += loss.item() * num_tokens\n","                    total_tokens += num_tokens\n","\n","                processed += 1\n","                if processed % 10 == 0:\n","                    print(f\"   Processados: {processed}/{len(sample_dataset)}\")\n","\n","                # Limpa memória\n","                del input_ids, labels, outputs\n","                cleanup_memory()\n","\n","            except Exception as e:\n","                print(f\"⚠️ Erro no exemplo {processed}: {e}\")\n","                continue\n","\n","    if total_tokens > 0:\n","        avg_loss = total_loss / total_tokens\n","        perplexity = np.exp(avg_loss)\n","    else:\n","        perplexity = float('inf')\n","\n","    return perplexity\n","\n","# Calcula perplexity do modelo treinado\n","print(\"\\n🔄 Calculando métricas do modelo treinado...\")\n","perplexity_trained = calcular_perplexity_otimizado(model_trained, tokenizer_trained, tokenized_val)\n","print(f\"📊 Perplexity modelo treinado: {perplexity_trained:.2f}\")\n","\n","# Remove modelo treinado da memória\n","del model_trained\n","cleanup_memory()\n","print(\"🧹 Modelo treinado removido da memória\")\n","\n","# === COMPARAÇÃO COM MODELO BASE ===\n","print(\"\\n🔄 Carregando modelo base para comparação...\")\n","try:\n","    model_base = AutoModelForCausalLM.from_pretrained(\n","        model_id,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\",\n","        low_cpu_mem_usage=True\n","    )\n","    model_base.eval()\n","\n","    # Testa modelo base\n","    print(\"🔄 Testando modelo base...\")\n","    pergunta_teste = \"Como se diz 'gavião' em Guarani?\"\n","    inputs = tokenizer(pergunta_teste, return_tensors=\"pt\")\n","\n","    with torch.no_grad():\n","        outputs_base = model_base.generate(\n","            inputs['input_ids'],\n","            max_length=80,\n","            do_sample=False,\n","            num_return_sequences=1,\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","\n","    resposta_base = tokenizer.decode(outputs_base[0], skip_special_tokens=True)\n","    resposta_base_limpa = resposta_base[len(pergunta_teste):].strip()\n","\n","    # Calcula perplexity do modelo base\n","    perplexity_base = calcular_perplexity_otimizado(model_base, tokenizer, tokenized_val)\n","\n","    # Resultados finais\n","    print(f\"\\n=== RESULTADOS FINAIS ===\")\n","    print(f\"📊 Perplexity modelo base: {perplexity_base:.2f}\")\n","    print(f\"📊 Perplexity modelo treinado: {perplexity_trained:.2f}\")\n","\n","    print(f\"\\n🔍 TESTE COMPARATIVO: '{pergunta_teste}'\")\n","    print(f\"Modelo Base: {resposta_base_limpa}\")\n","\n","    # Carrega modelo treinado novamente para comparação\n","    model_trained_comp = AutoModelForCausalLM.from_pretrained(\n","        model_path,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\",\n","        low_cpu_mem_usage=True\n","    )\n","\n","    with torch.no_grad():\n","        outputs_trained = model_trained_comp.generate(\n","            inputs['input_ids'],\n","            max_length=80,\n","            do_sample=False,\n","            num_return_sequences=1,\n","            pad_token_id=tokenizer_trained.eos_token_id\n","        )\n","\n","    resposta_trained = tokenizer_trained.decode(outputs_trained[0], skip_special_tokens=True)\n","    resposta_trained_limpa = resposta_trained[len(pergunta_teste):].strip()\n","    print(f\"Modelo Treinado: {resposta_trained_limpa}\")\n","\n","    if perplexity_trained < perplexity_base:\n","        melhoria = perplexity_base - perplexity_trained\n","        print(f\"\\n✅ SUCESSO! O fine-tuning melhorou o modelo!\")\n","        print(f\"🎯 Redução na perplexity: {melhoria:.2f}\")\n","    else:\n","        diferenca = perplexity_trained - perplexity_base\n","        print(f\"\\n⚠️ O modelo ainda precisa de ajustes\")\n","        print(f\"📈 Aumento na perplexity: {diferenca:.2f}\")\n","        print(\"💡 Sugestões: mais dados, mais épocas, ou ajustar learning rate\")\n","\n","except Exception as e:\n","    print(f\"⚠️ Erro na comparação: {e}\")\n","    print(\"✅ Mas o treinamento foi concluído com sucesso!\")\n","\n","# Limpeza final\n","cleanup_memory()\n","print(f\"\\n🎯 Avaliação concluída! Modelo salvo em: {output_dir}\")\n","print_gpu_utilization()"]}]}