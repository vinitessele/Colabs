{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyMgIn33qvPPBHSMfmOIl7SK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0899a043970f4cb288a26c3f86178424":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b581717d355c4a618e78724b7acc218d","IPY_MODEL_e27c28b2ea834b65a06692f9843df1ab","IPY_MODEL_ce5f4fcc3a274a99ae0107580d35b94f"],"layout":"IPY_MODEL_9e4819544c134cb98bb234d0f1848a80"}},"b581717d355c4a618e78724b7acc218d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_352cb2e334fb42d1b135f10fd8ddac01","placeholder":"‚Äã","style":"IPY_MODEL_b4fcceaea5f34c92a379694cc9d863f5","value":"Loading‚Äácheckpoint‚Äáshards:‚Äá100%"}},"e27c28b2ea834b65a06692f9843df1ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4b11aac23c8402b9892bd515d67b524","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e44d8cb1cf9448b9d3a0e8bc0ac4fda","value":2}},"ce5f4fcc3a274a99ae0107580d35b94f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cab9080815da469eae79dc01df4437af","placeholder":"‚Äã","style":"IPY_MODEL_35affd4c71744d79b2cce97558437a49","value":"‚Äá2/2‚Äá[00:01&lt;00:00,‚Äá‚Äá1.69s/it]"}},"9e4819544c134cb98bb234d0f1848a80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"352cb2e334fb42d1b135f10fd8ddac01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4fcceaea5f34c92a379694cc9d863f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4b11aac23c8402b9892bd515d67b524":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e44d8cb1cf9448b9d3a0e8bc0ac4fda":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cab9080815da469eae79dc01df4437af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35affd4c71744d79b2cce97558437a49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"920736d9e0f748f2aecac60b3a0fbe4e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3f5b003f27aa41bb9328f917711241da","IPY_MODEL_1d67739e532747178dd71feb07eaa57e","IPY_MODEL_8f35a15e89254ad39f88d71cb85cda23"],"layout":"IPY_MODEL_d4c935f775524d4ea50fe668f42ca9e8"}},"3f5b003f27aa41bb9328f917711241da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4373dd5a5a34d25aaedf1a657e3e435","placeholder":"‚Äã","style":"IPY_MODEL_c2dcb9d702864c8192b166aac611fb97","value":"Map:‚Äá100%"}},"1d67739e532747178dd71feb07eaa57e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cf22c2b1cad41c4a069804a7b0d5aeb","max":224,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b96c9d0c82bf4790b852c2de95d5d73a","value":224}},"8f35a15e89254ad39f88d71cb85cda23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65812b73bcf54e9d8e644cc5208dee34","placeholder":"‚Äã","style":"IPY_MODEL_94420943149e46f2b98cef6ff36501f9","value":"‚Äá224/224‚Äá[00:00&lt;00:00,‚Äá2091.75‚Äáexamples/s]"}},"d4c935f775524d4ea50fe668f42ca9e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4373dd5a5a34d25aaedf1a657e3e435":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2dcb9d702864c8192b166aac611fb97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cf22c2b1cad41c4a069804a7b0d5aeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b96c9d0c82bf4790b852c2de95d5d73a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65812b73bcf54e9d8e644cc5208dee34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94420943149e46f2b98cef6ff36501f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7f6fbbcc5bd49c9a33bba44882aa6ae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c959181174843708a4addec71c5e394","IPY_MODEL_63aae387b09b4511817b99f1d38699cf","IPY_MODEL_2be56c8197d74fb09d98ea41b9fccb5c"],"layout":"IPY_MODEL_636e24618c3c4bed91218fe6f89f12a0"}},"4c959181174843708a4addec71c5e394":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f08d4a8bdb74964aa155f8421c53102","placeholder":"‚Äã","style":"IPY_MODEL_2b5c1ca4f9454f52a5310c505137af1a","value":"Map:‚Äá100%"}},"63aae387b09b4511817b99f1d38699cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72bca3ee35f84a77896b59907f105e4e","max":57,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ba6dea8e42c5447a8d5e99a974928025","value":57}},"2be56c8197d74fb09d98ea41b9fccb5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_222b611d48b74c1c92177297d3381faf","placeholder":"‚Äã","style":"IPY_MODEL_473395f1ffe84e97bcc0353da8deae49","value":"‚Äá57/57‚Äá[00:00&lt;00:00,‚Äá1565.42‚Äáexamples/s]"}},"636e24618c3c4bed91218fe6f89f12a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f08d4a8bdb74964aa155f8421c53102":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b5c1ca4f9454f52a5310c505137af1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72bca3ee35f84a77896b59907f105e4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba6dea8e42c5447a8d5e99a974928025":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"222b611d48b74c1c92177297d3381faf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"473395f1ffe84e97bcc0353da8deae49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0899a043970f4cb288a26c3f86178424","b581717d355c4a618e78724b7acc218d","e27c28b2ea834b65a06692f9843df1ab","ce5f4fcc3a274a99ae0107580d35b94f","9e4819544c134cb98bb234d0f1848a80","352cb2e334fb42d1b135f10fd8ddac01","b4fcceaea5f34c92a379694cc9d863f5","f4b11aac23c8402b9892bd515d67b524","3e44d8cb1cf9448b9d3a0e8bc0ac4fda","cab9080815da469eae79dc01df4437af","35affd4c71744d79b2cce97558437a49","920736d9e0f748f2aecac60b3a0fbe4e","3f5b003f27aa41bb9328f917711241da","1d67739e532747178dd71feb07eaa57e","8f35a15e89254ad39f88d71cb85cda23","d4c935f775524d4ea50fe668f42ca9e8","d4373dd5a5a34d25aaedf1a657e3e435","c2dcb9d702864c8192b166aac611fb97","3cf22c2b1cad41c4a069804a7b0d5aeb","b96c9d0c82bf4790b852c2de95d5d73a","65812b73bcf54e9d8e644cc5208dee34","94420943149e46f2b98cef6ff36501f9","e7f6fbbcc5bd49c9a33bba44882aa6ae","4c959181174843708a4addec71c5e394","63aae387b09b4511817b99f1d38699cf","2be56c8197d74fb09d98ea41b9fccb5c","636e24618c3c4bed91218fe6f89f12a0","3f08d4a8bdb74964aa155f8421c53102","2b5c1ca4f9454f52a5310c505137af1a","72bca3ee35f84a77896b59907f105e4e","ba6dea8e42c5447a8d5e99a974928025","222b611d48b74c1c92177297d3381faf","473395f1ffe84e97bcc0353da8deae49"]},"id":"S8hPQ6iN_KtC","executionInfo":{"status":"error","timestamp":1751138921574,"user_tz":180,"elapsed":9156,"user":{"displayName":"Vinicius Tessele","userId":"00998660620187565274"}},"outputId":"1c7981d3-66d5-4d28-cee3-77153a367bd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","üîÑ Carregando modelo: Emilio407/guarani-jopara-gemma-2-2b-it-v1\n","‚úÖ Tokenizer carregado\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0899a043970f4cb288a26c3f86178424"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Modelo carregado com otimiza√ß√µes\n","üñ•Ô∏è GPU - Alocado: 14.6GB | Reservado: 15.7GB\n","\n","üîÑ Carregando e preparando dados...\n","üìä Treino: 224 | Valida√ß√£o: 57\n","üîÑ Aplicando tokeniza√ß√£o...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/224 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"920736d9e0f748f2aecac60b3a0fbe4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/57 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f6fbbcc5bd49c9a33bba44882aa6ae"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Tokeniza√ß√£o conclu√≠da\n","\n","üîÑ Iniciando treinamento com configura√ß√µes otimizadas...\n","üñ•Ô∏è GPU - Alocado: 14.6GB | Reservado: 15.7GB\n","üöÄ Iniciando treinamento...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2-5183507.py:210: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"stream","name":"stdout","text":["‚ùå Erro durante treinamento: CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 22.16 GiB of which 1.03 GiB is free. Process 21898 has 21.13 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 304.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","üí° Tentando com configura√ß√µes ainda mais conservadoras...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2-5183507.py:241: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 22.16 GiB of which 1.03 GiB is free. Process 21898 has 21.13 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 304.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2-5183507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üöÄ Iniciando treinamento...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Treinamento conclu√≠do!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2548\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2549\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2550\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 22.16 GiB of which 1.03 GiB is free. Process 21898 has 21.13 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 304.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2-5183507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3789\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2547\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2549\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2550\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2551\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 22.16 GiB of which 1.03 GiB is free. Process 21898 has 21.13 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 304.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n","import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","from torch.nn import CrossEntropyLoss\n","import gc\n","\n","# === CONFIGURA√á√ÉO OTIMIZADA DE MEM√ìRIA ===\n","def cleanup_memory():\n","    \"\"\"Limpa cache da GPU e coleta garbage\"\"\"\n","    gc.collect()\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","        torch.cuda.synchronize()\n","\n","def print_gpu_utilization():\n","    \"\"\"Mostra utiliza√ß√£o da GPU\"\"\"\n","    if torch.cuda.is_available():\n","        allocated = torch.cuda.memory_allocated() / 1024**3\n","        reserved = torch.cuda.memory_reserved() / 1024**3\n","        print(f\"üñ•Ô∏è GPU - Alocado: {allocated:.1f}GB | Reservado: {reserved:.1f}GB\")\n","\n","# Configura√ß√µes globais para economia de mem√≥ria\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","# Limpeza inicial\n","cleanup_memory()\n","\n","drive.mount('/content/drive')\n","\n","# === CONFIGURA√á√ÉO DO MODELO ===\n","model_id = \"Emilio407/guarani-jopara-gemma-2-2b-it-v1\"\n","print(f\"üîÑ Carregando modelo: {model_id}\")\n","\n","# Carrega tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","print(\"‚úÖ Tokenizer carregado\")\n","\n","# Carrega modelo com otimiza√ß√µes\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float16,  # Usa half precision desde o carregamento\n","    device_map=\"auto\",\n","    low_cpu_mem_usage=True,\n",")\n","print(\"‚úÖ Modelo carregado com otimiza√ß√µes\")\n","print_gpu_utilization()\n","\n","# === PREPARA√á√ÉO DOS DADOS OTIMIZADA ===\n","print(\"\\nüîÑ Carregando e preparando dados...\")\n","df = pd.read_json(\"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/dados_treinamento_guarani.json\")\n","\n","# Divide os dados (usando menos dados se necess√°rio para economizar mem√≥ria)\n","if len(df) > 1000:  # Se tiver muitos dados, usa uma amostra\n","    df = df.sample(n=1000, random_state=42)\n","    print(f\"‚ö†Ô∏è Usando amostra de 1000 exemplos para economizar mem√≥ria\")\n","\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n","print(f\"üìä Treino: {len(train_df)} | Valida√ß√£o: {len(val_df)}\")\n","\n","# Converte para Dataset\n","train_dataset = Dataset.from_pandas(train_df)\n","val_dataset = Dataset.from_pandas(val_df)\n","\n","# Fun√ß√£o de tokeniza√ß√£o otimizada\n","def tokenize(example):\n","    prompt = example[\"instruction\"] + \"\\n\" + example[\"input\"] + \"\\n\"\n","    target = example[\"output\"]\n","    full_text = prompt + target\n","\n","    # Tokeniza com max_length menor para economizar mem√≥ria\n","    tokenized = tokenizer(\n","        full_text,\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=256,  # Reduzido para economizar mem√≥ria\n","        return_tensors=None  # N√£o retorna tensors para economizar mem√≥ria\n","    )\n","\n","    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n","    return tokenized\n","\n","print(\"üîÑ Aplicando tokeniza√ß√£o...\")\n","tokenized_train = train_dataset.map(\n","    tokenize,\n","    remove_columns=train_dataset.column_names,\n","    batched=False,  # Processa um por vez para economizar mem√≥ria\n","    load_from_cache_file=False\n",")\n","tokenized_val = val_dataset.map(\n","    tokenize,\n","    remove_columns=val_dataset.column_names,\n","    batched=False,\n","    load_from_cache_file=False\n",")\n","\n","cleanup_memory()\n","print(\"‚úÖ Tokeniza√ß√£o conclu√≠da\")\n","\n","# Data collator otimizado\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n","    return_tensors=\"pt\"\n",")\n","\n","# === FUN√á√ÉO DE M√âTRICAS OTIMIZADA ===\n","def compute_metrics(eval_pred):\n","    \"\"\"Fun√ß√£o de m√©tricas otimizada para economizar mem√≥ria\"\"\"\n","    predictions, labels = eval_pred\n","\n","    # Move para CPU se necess√°rio\n","    if isinstance(predictions, torch.Tensor):\n","        predictions = predictions.detach().cpu()\n","    if isinstance(labels, torch.Tensor):\n","        labels = labels.detach().cpu()\n","\n","    # Calcula perplexity de forma mais eficiente\n","    try:\n","        shift_logits = predictions[..., :-1, :].contiguous()\n","        shift_labels = labels[..., 1:].contiguous()\n","\n","        shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n","        shift_labels = shift_labels.view(-1)\n","\n","        # Remove padding\n","        mask = shift_labels != -100\n","        shift_logits = shift_logits[mask]\n","        shift_labels = shift_labels[mask]\n","\n","        # Calcula loss\n","        loss_fct = CrossEntropyLoss()\n","        loss = loss_fct(shift_logits.float(), shift_labels)\n","        perplexity = torch.exp(loss)\n","\n","        return {\n","            \"eval_loss\": loss.item(),\n","            \"perplexity\": perplexity.item()\n","        }\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è Erro no c√°lculo de m√©tricas: {e}\")\n","        return {\"eval_loss\": 0.0, \"perplexity\": 1000.0}\n","\n","# === ARGUMENTOS DE TREINAMENTO ULTRA-OTIMIZADOS ===\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/modelo_finetunado\",\n","\n","    # Configura√ß√µes de batch otimizadas\n","    per_device_train_batch_size=1,      # Muito pequeno para economizar mem√≥ria\n","    per_device_eval_batch_size=1,       # Muito pequeno para avalia√ß√£o\n","    gradient_accumulation_steps=8,       # Simula batch_size=8\n","\n","    # Configura√ß√µes de treinamento\n","    num_train_epochs=2,                  # Reduzido para teste inicial\n","    learning_rate=2e-5,                  # Learning rate conservador\n","    warmup_ratio=0.1,                    # Warmup para estabilidade\n","\n","    # Configura√ß√µes de avalia√ß√£o\n","    eval_strategy=\"steps\",\n","    eval_steps=100,                      # Avalia menos frequentemente\n","    eval_accumulation_steps=1,           # N√£o acumula durante avalia√ß√£o\n","\n","    # Configura√ß√µes de salvamento\n","    save_strategy=\"steps\",\n","    save_steps=200,                      # Salva menos frequentemente\n","    save_total_limit=1,                  # Mant√©m apenas 1 checkpoint\n","\n","    # Otimiza√ß√µes de mem√≥ria CR√çTICAS\n","    fp16=False,                          # Half precision - ESSENCIAL!\n","    dataloader_pin_memory=False,        # Economiza mem√≥ria\n","    gradient_checkpointing=True,        # Troca computa√ß√£o por mem√≥ria\n","    dataloader_num_workers=0,           # Sem workers paralelos\n","    remove_unused_columns=True,         # Remove colunas desnecess√°rias\n","\n","    # Configura√ß√µes de otimizador\n","    optim=\"adamw_torch\",                # Otimizador eficiente\n","    weight_decay=0.01,\n","    adam_epsilon=1e-8,\n","\n","    # Logging e relat√≥rios\n","    logging_dir=\"logs\",\n","    logging_steps=50,\n","    logging_first_step=True,\n","\n","    # Configura√ß√µes de modelo\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    greater_is_better=False,\n","\n","    # Desabilita relat√≥rios externos\n","    report_to=[],\n","\n","    # Configura√ß√µes adicionais para estabilidade\n","    max_grad_norm=1.0,                  # Gradient clipping\n","    prediction_loss_only=False,\n",")\n","\n","print(\"\\nüîÑ Iniciando treinamento com configura√ß√µes otimizadas...\")\n","print_gpu_utilization()\n","\n","# === TRAINER OTIMIZADO ===\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_val,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# === TREINAMENTO COM MONITORAMENTO ===\n","try:\n","    print(\"üöÄ Iniciando treinamento...\")\n","    trainer.train()\n","    print(\"‚úÖ Treinamento conclu√≠do!\")\n","\n","    # Salva o modelo\n","    output_dir = \"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/modelo_finetunado\"\n","    trainer.save_model(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","    print(f\"‚úÖ Modelo salvo em: {output_dir}\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Erro durante treinamento: {e}\")\n","    print(\"üí° Tentando com configura√ß√µes ainda mais conservadoras...\")\n","\n","    # Configura√ß√µes de emerg√™ncia\n","    training_args.per_device_train_batch_size = 1\n","    training_args.gradient_accumulation_steps = 4\n","    training_args.num_train_epochs = 1\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train.select(range(min(100, len(tokenized_train)))),  # Usa s√≥ 100 exemplos\n","        eval_dataset=tokenized_val.select(range(min(20, len(tokenized_val)))),\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","    )\n","\n","    trainer.train()\n","    trainer.save_model(output_dir)\n","\n","# === LIMPEZA ANTES DA AVALIA√á√ÉO ===\n","del trainer\n","cleanup_memory()\n","print(\"\\nüßπ Mem√≥ria limpa ap√≥s treinamento\")\n","print_gpu_utilization()\n","\n","# === AVALIA√á√ÉO OTIMIZADA ===\n","print(\"\\n=== INICIANDO AVALIA√á√ÉO ===\")\n","\n","# Carrega modelo treinado com otimiza√ß√µes\n","model_path = \"/content/drive/MyDrive/Doutorado Unesp/assistente-guarani/data/modelo_finetunado\"\n","tokenizer_trained = AutoTokenizer.from_pretrained(model_path)\n","model_trained = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    low_cpu_mem_usage=True\n",")\n","model_trained.eval()\n","\n","print(\"‚úÖ Modelo treinado carregado para avalia√ß√£o\")\n","\n","# Fun√ß√£o de avalia√ß√£o otimizada\n","def avaliar_modelo_otimizado(perguntas_teste):\n","    \"\"\"Avalia o modelo de forma otimizada\"\"\"\n","    resultados = []\n","\n","    for i, pergunta in enumerate(perguntas_teste):\n","        try:\n","            inputs = tokenizer_trained(pergunta, return_tensors=\"pt\")\n","\n","            with torch.no_grad():\n","                outputs = model_trained.generate(\n","                    inputs['input_ids'],\n","                    max_length=80,  # Reduzido para economizar\n","                    do_sample=True,\n","                    temperature=0.7,\n","                    num_return_sequences=1,\n","                    pad_token_id=tokenizer_trained.eos_token_id,\n","                    early_stopping=True\n","                )\n","\n","            resposta = tokenizer_trained.decode(outputs[0], skip_special_tokens=True)\n","            resposta_limpa = resposta[len(pergunta):].strip()\n","\n","            resultados.append({\n","                'pergunta': pergunta,\n","                'resposta': resposta_limpa\n","            })\n","\n","            print(f\"Pergunta {i+1}: {pergunta}\")\n","            print(f\"Resposta: {resposta_limpa}\")\n","            print(\"-\" * 50)\n","\n","            # Limpa mem√≥ria a cada itera√ß√£o\n","            del inputs, outputs\n","            cleanup_memory()\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Erro na pergunta '{pergunta}': {e}\")\n","\n","    return resultados\n","\n","# Perguntas de teste\n","perguntas_teste = [\n","    \"Como se diz 'gavi√£o' em Guarani?\",\n","    \"Como se diz '√°gua' em Guarani?\",\n","    \"Como se diz 'casa' em Guarani?\",\n","    \"Como se diz 'sol' em Guarani?\",\n","    \"Como se diz 'lua' em Guarani?\"\n","]\n","\n","print(\"üîÑ Testando modelo treinado...\")\n","resultados = avaliar_modelo_otimizado(perguntas_teste)\n","\n","# === C√ÅLCULO DE PERPLEXITY OTIMIZADO ===\n","def calcular_perplexity_otimizado(model, tokenizer, dataset, max_samples=30):\n","    \"\"\"Calcula perplexity de forma otimizada\"\"\"\n","    model.eval()\n","    total_loss = 0\n","    total_tokens = 0\n","    processed = 0\n","\n","    # Usa amostra muito pequena para economizar mem√≥ria\n","    indices = np.random.choice(len(dataset), min(max_samples, len(dataset)), replace=False)\n","    sample_dataset = dataset.select(indices)\n","\n","    print(f\"üîÑ Calculando perplexity em {len(sample_dataset)} exemplos...\")\n","\n","    with torch.no_grad():\n","        for example in sample_dataset:\n","            try:\n","                input_ids = torch.tensor([example['input_ids']], dtype=torch.long)\n","                labels = torch.tensor([example['labels']], dtype=torch.long)\n","\n","                outputs = model(input_ids, labels=labels)\n","                loss = outputs.loss\n","\n","                # Conta tokens v√°lidos\n","                mask = labels != -100\n","                num_tokens = mask.sum().item()\n","\n","                if num_tokens > 0:\n","                    total_loss += loss.item() * num_tokens\n","                    total_tokens += num_tokens\n","\n","                processed += 1\n","                if processed % 10 == 0:\n","                    print(f\"   Processados: {processed}/{len(sample_dataset)}\")\n","\n","                # Limpa mem√≥ria\n","                del input_ids, labels, outputs\n","                cleanup_memory()\n","\n","            except Exception as e:\n","                print(f\"‚ö†Ô∏è Erro no exemplo {processed}: {e}\")\n","                continue\n","\n","    if total_tokens > 0:\n","        avg_loss = total_loss / total_tokens\n","        perplexity = np.exp(avg_loss)\n","    else:\n","        perplexity = float('inf')\n","\n","    return perplexity\n","\n","# Calcula perplexity do modelo treinado\n","print(\"\\nüîÑ Calculando m√©tricas do modelo treinado...\")\n","perplexity_trained = calcular_perplexity_otimizado(model_trained, tokenizer_trained, tokenized_val)\n","print(f\"üìä Perplexity modelo treinado: {perplexity_trained:.2f}\")\n","\n","# Remove modelo treinado da mem√≥ria\n","del model_trained\n","cleanup_memory()\n","print(\"üßπ Modelo treinado removido da mem√≥ria\")\n","\n","# === COMPARA√á√ÉO COM MODELO BASE ===\n","print(\"\\nüîÑ Carregando modelo base para compara√ß√£o...\")\n","try:\n","    model_base = AutoModelForCausalLM.from_pretrained(\n","        model_id,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\",\n","        low_cpu_mem_usage=True\n","    )\n","    model_base.eval()\n","\n","    # Testa modelo base\n","    print(\"üîÑ Testando modelo base...\")\n","    pergunta_teste = \"Como se diz 'gavi√£o' em Guarani?\"\n","    inputs = tokenizer(pergunta_teste, return_tensors=\"pt\")\n","\n","    with torch.no_grad():\n","        outputs_base = model_base.generate(\n","            inputs['input_ids'],\n","            max_length=80,\n","            do_sample=False,\n","            num_return_sequences=1,\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","\n","    resposta_base = tokenizer.decode(outputs_base[0], skip_special_tokens=True)\n","    resposta_base_limpa = resposta_base[len(pergunta_teste):].strip()\n","\n","    # Calcula perplexity do modelo base\n","    perplexity_base = calcular_perplexity_otimizado(model_base, tokenizer, tokenized_val)\n","\n","    # Resultados finais\n","    print(f\"\\n=== RESULTADOS FINAIS ===\")\n","    print(f\"üìä Perplexity modelo base: {perplexity_base:.2f}\")\n","    print(f\"üìä Perplexity modelo treinado: {perplexity_trained:.2f}\")\n","\n","    print(f\"\\nüîç TESTE COMPARATIVO: '{pergunta_teste}'\")\n","    print(f\"Modelo Base: {resposta_base_limpa}\")\n","\n","    # Carrega modelo treinado novamente para compara√ß√£o\n","    model_trained_comp = AutoModelForCausalLM.from_pretrained(\n","        model_path,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\",\n","        low_cpu_mem_usage=True\n","    )\n","\n","    with torch.no_grad():\n","        outputs_trained = model_trained_comp.generate(\n","            inputs['input_ids'],\n","            max_length=80,\n","            do_sample=False,\n","            num_return_sequences=1,\n","            pad_token_id=tokenizer_trained.eos_token_id\n","        )\n","\n","    resposta_trained = tokenizer_trained.decode(outputs_trained[0], skip_special_tokens=True)\n","    resposta_trained_limpa = resposta_trained[len(pergunta_teste):].strip()\n","    print(f\"Modelo Treinado: {resposta_trained_limpa}\")\n","\n","    if perplexity_trained < perplexity_base:\n","        melhoria = perplexity_base - perplexity_trained\n","        print(f\"\\n‚úÖ SUCESSO! O fine-tuning melhorou o modelo!\")\n","        print(f\"üéØ Redu√ß√£o na perplexity: {melhoria:.2f}\")\n","    else:\n","        diferenca = perplexity_trained - perplexity_base\n","        print(f\"\\n‚ö†Ô∏è O modelo ainda precisa de ajustes\")\n","        print(f\"üìà Aumento na perplexity: {diferenca:.2f}\")\n","        print(\"üí° Sugest√µes: mais dados, mais √©pocas, ou ajustar learning rate\")\n","\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è Erro na compara√ß√£o: {e}\")\n","    print(\"‚úÖ Mas o treinamento foi conclu√≠do com sucesso!\")\n","\n","# Limpeza final\n","cleanup_memory()\n","print(f\"\\nüéØ Avalia√ß√£o conclu√≠da! Modelo salvo em: {output_dir}\")\n","print_gpu_utilization()"]}]}