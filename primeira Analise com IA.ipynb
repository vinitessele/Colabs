{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (0.19.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (4.46.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.54.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vinic\\appdata\\roaming\\python\\python312\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers matplotlib seaborn scikit-learn pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Diret칩rios\n",
    "base_dir = \"D:/Mamografias Coenc/Abril\"\n",
    "images_dir = os.path.join(base_dir, \"images\")\n",
    "reports_dir = os.path.join(base_dir, \"reports\")\n",
    "\n",
    "# Par칙metros\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Tokenizer para laudos\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MamografiaDataset(Dataset):\n",
    "    def __init__(self, images_dir, reports_dir, transform=None, tokenizer=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.reports_dir = reports_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.images = sorted(os.listdir(images_dir))\n",
    "        self.reports = sorted(os.listdir(reports_dir))\n",
    "        \n",
    "        print(f\"Total de imagens: {len(self.images)}\")\n",
    "        print(f\"Total de relat칩rios: {len(self.reports)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.images) or idx >= len(self.reports):  # Verifica칞칚o de 칤ndice v치lido\n",
    "            print(f\"丘멆잺 칈ndice inv치lido: {idx}. Tamanho m치ximo: {len(self.images)-1}\")\n",
    "            return None  # Evita erro ao tentar acessar um 칤ndice inexistente\n",
    "        \n",
    "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
    "        txt_path = os.path.join(self.reports_dir, self.reports[idx])\n",
    "        \n",
    "        print(f\"游댳 Carregando imagem: {img_path}\")\n",
    "        print(f\"游댳 Carregando relat칩rio: {txt_path}\")\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        text = load_text(txt_path)  # Usa a fun칞칚o segura de leitura\n",
    "        inputs = self.tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        label = int(\"normal\" not in text.lower())  # Exemplo de categoriza칞칚o bin치ria (0: Normal, 1: Alterado)\n",
    "\n",
    "        return image, inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path):\n",
    "    \"\"\"L칡 o arquivo de texto tentando v치rias codifica칞칫es.\"\"\"\n",
    "    encodings = ['utf-8', 'ISO-8859-1', 'windows-1252']\n",
    "    \n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=enc) as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue  # Tenta a pr칩xima codifica칞칚o\n",
    "\n",
    "    print(f\"丘멆잺 Erro ao ler {file_path} - Nenhuma codifica칞칚o funcionou.\")\n",
    "    return \"\"  # Retorna string vazia se falhar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de imagens: 215\n",
      "Total de relat칩rios: 52\n"
     ]
    }
   ],
   "source": [
    "# Transforma칞칚o de imagens\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = MamografiaDataset(images_dir, reports_dir, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from transformers import BertModel\n",
    "\n",
    "class MultiModalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiModalNet, self).__init__()\n",
    "\n",
    "        # Modelo CNN (ResNet18)\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        in_features = self.cnn.fc.in_features  # Obt칠m a dimens칚o de sa칤da da ResNet\n",
    "        self.cnn.fc = nn.Linear(in_features, 256)\n",
    "\n",
    "        # Modelo BERT\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.text_fc = nn.Linear(768, 256)\n",
    "\n",
    "        # Camada final de classifica칞칚o\n",
    "        self.classifier = nn.Linear(256 + 256, 2)\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        img_features = self.cnn(image)\n",
    "        text_features = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        text_features = self.text_fc(text_features)\n",
    "\n",
    "        combined = torch.cat((img_features, text_features), dim=1)\n",
    "        output = self.classifier(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa modelo, otimizador e loss\n",
    "model = MultiModalNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Treinamento\n",
    "def train(model, dataloader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, input_ids, attention_mask, labels in dataloader:\n",
    "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "丘멆잺 칈ndice inv치lido: 62. Tamanho m치ximo: 214\n",
      "丘멆잺 칈ndice inv치lido: 203. Tamanho m치ximo: 214\n",
      "丘멆잺 칈ndice inv치lido: 155. Tamanho m치ximo: 214\n",
      "丘멆잺 칈ndice inv치lido: 120. Tamanho m치ximo: 214\n",
      "丘멆잺 칈ndice inv치lido: 84. Tamanho m치ximo: 214\n",
      "游댳 Carregando imagem: D:/Mamografias Coenc/Abril\\images\\32867_32060_3.png\n",
      "游댳 Carregando relat칩rio: D:/Mamografias Coenc/Abril\\reports\\33383_32616.pdf\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_true, y_pred\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Treinar modelo\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Definir diret칩rios\u001b[39;00m\n\u001b[0;32m     20\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Mamografias Coenc/Abril\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[33], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     13\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[29], line 33\u001b[0m, in \u001b[0;36mMamografiaDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     30\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m     32\u001b[0m text \u001b[38;5;241m=\u001b[39m load_text(txt_path)  \u001b[38;5;66;03m# Usa a fun칞칚o segura de leitura\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39mlower())  \u001b[38;5;66;03m# Exemplo de categoriza칞칚o bin치ria (0: Normal, 1: Alterado)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor(label)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# Avalia칞칚o\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask, labels in dataloader:\n",
    "            images, input_ids, attention_mask, labels = (\n",
    "                images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            )\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            y_true.extend(labels.cpu().numpy())  # Garante que labels est치 na CPU antes de converter para NumPy\n",
    "            y_pred.extend(predictions)\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Treinar modelo\n",
    "train(model, dataloader, criterion, optimizer, EPOCHS)\n",
    "\n",
    "# Definir diret칩rios\n",
    "base_dir = \"D:/Mamografias Coenc/Abril\"\n",
    "images_dir = os.path.join(base_dir, \"images\")\n",
    "reports_dir = os.path.join(base_dir, \"reports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia칞칚o e matriz de confus칚o\n",
    "y_true, y_pred = evaluate(model, dataloader)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Alterado'], yticklabels=['Normal', 'Alterado'])\n",
    "plt.xlabel(\"Predito\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confus칚o\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\", classification_report(y_true, y_pred, target_names=['Normal', 'Alterado']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
